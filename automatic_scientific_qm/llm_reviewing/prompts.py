"""
Prompt constants used for the Sakana LLM reviewer.
"""

"""
Part of the Code in this file is adapted from the AI-Scientist project by SakanaAI (https://github.com/SakanaAI/AI-Scientist), licensed under the Apache License, Version 2.0.

Copyright 2024 SakanaAI

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

# Data for Paper 1 of ICLR-2023
title_iclr_2023_1 = (
    "Title: Sample-efficient multi-objective molecular optimization with GFlowNets"
)
abstract_iclr_2023_1 = "Abstract: Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as an expensive black-box optimization problem over the discrete chemical space. Computational methods have achieved initial success but still struggle with simultaneously optimizing multiple competing properties in a sample-efficient manner. In this work, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. Inspired by reinforcement learning, we further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed up learning for HN-GFN. Through synthetic experiments, we illustrate that HN-GFN has adequate capacity to generalize over preferences. Extensive experiments show that our framework outperforms the best baselines by a large margin in terms of hypervolume in various real-world MOBO settings."
introduction_iclr_2023_1 = "Introduction: Designing novel molecular structures with desired properties, also referred to as molecular optimization, is a crucial task with great application potential in scientific fields ranging from drug discovery to material design. Molecular optimization can be naturally formulated as a black-box optimization problem over the discrete chemical space, which is combinatorially large (Polishchuk et al., 2013) . Recent years have witnessed the trend of leveraging computational methods, such as deep generative models (Jin et al., 2018) and combinatorial optimization algorithms (You et al., 2018; Jensen, 2019) , to facilitate the optimization. However, the applicability of most prior approaches in real-world scenarios is hindered by two practical constraints: (i) realistic oracles (e.g., wet-lab experiments and high-fidelity simulations) require substantial costs to synthesize and evaluate molecules (Gao et al., 2022) , and (ii) chemists commonly seek to optimize multiple properties of interest simultaneously (Jin et al., 2020b) . For example, in addition to effectively inhibiting a disease-associated target, an ideal drug is desired to be easily synthesizable and non-toxic.Bayesian optimization (BO) (Jones et al., 1998; Shahriari et al., 2015) provides a sample-efficient framework for globally optimizing expensive black-box functions. The basic idea is to construct a cheap-to-evaluate surrogate model, typically a Gaussian Process (GP) (Rasmussen, 2003) , to approximate the true function (also known as the oracle) on the observed dataset. The core objective of BO is to optimize an acquisition function (built upon the surrogate model) in order to obtain informative candidates with high utility for the next round of evaluations. This loop is repeated until the evaluation budget is exhausted. Owing to the fact that a large batch of candidates can be evaluated in parallel in biochemical experiments, we perform batch BO (with large-batch and low-round settings (Angermueller et al., 2020) ) to significantly shorten the entire cycle of optimization.As multi-objective optimization (MOO) problems are prevalent in scientific and engineering applications, MOBO also received broad attention and achieved promising performance by effectively optimizing differentiable acquisition functions (Daulton et al., 2020) . Nevertheless, it is less prominent in discrete problems, especially considering batch settings. The difficulty lies in the fact that no gradients can be leveraged to navigate the discrete space for efficient and effective optimization of the acquisition function. Although most of the existing discrete molecular optimization methods can be adopted as the acquisition function optimizer to alleviate this issue, they suffer from the following limitations. 1) Most approaches do not explicitly discuss the diversity of the proposed candidates, which is a key consideration in batch settings as the surrogate model cannot exactly reproduce the oracle's full behaviors. Therefore, we not only want to cover more high modes of the surrogate model but also to obtain candidates that bring additional information about the search space. 2) Most multi-objective methods (Xie et al., 2021; Fu et al., 2022) simply rely on a scalarization function, parameterized by a predefined preference vector reflecting the trade-off between objectives, and turn the MOO problem into a single-objective one. Unfortunately, an ideal trade-off is unclear before optimization (even with domain knowledge), and many potential trade-offs of interest are worth exploring. In principle, it is possible to independently train multiple optimization models, each conditioned on a distinct preference vector, to cover the objective space. Practically, this trivial strategy cannot efficiently scale with the number of objectives (Navon et al., 2021) .The recently proposed GFlowNets (Bengio et al., 2021a) are a class of generative models over discrete objects (e.g., molecular graphs) that aim to learn a stochastic policy for sequentially constructing objects with a probability proportional to a reward function (e.g., the acquisition function). Hence, GFlowNets possess merit in generating diverse and high-reward objects, which makes them appealing in the batch BO context where exploration plays a significant role (Jain et al., 2022) .In this work, we present a MOBO algorithm based on GFlowNets for sample-efficient multiobjective molecular optimization. We propose a hypernetwork-based GFlowNet (HN-GFN) as the acquisition function optimizer within MOBO to sample a diverse batch of candidates from an approximate Pareto front. Instead of defining a fixed reward function as usual in past work (Bengio et al., 2021a) , we train a unified GFlowNet on the distribution of reward functions (random scalarizations parameterized by preference vectors) and control the policy using a single preferenceconditioned hypernetwork. While sampling candidates, HN-GFN explores various trade-offs between competing objectives flexibly by varying the input preference vector. Inspired by Hindsight Experience Replay (Andrychowicz et al., 2017) in RL, we further introduce a hindsight-like offpolicy strategy to share high-performing molecules among different preferences and speed up learning for HN-GFN. As detailed in our reported experiments, we first evaluate HN-GFN through synthetic experiments to verify that HN-GFN is capable of generalizing over preference vectors, then apply the proposed framework to real-world scenarios. Remarkably, our framework outperforms the best baselines by 60% and 24% (relative improvement in terms of hypervolume in the settings with two and four objectives), respectively. Our key contributions are summarized below:\u2022 We propose HN-GFN, a unified GFlowNet that can efficiently sample candidates from an approximate Pareto front using a single hypernetwork.\u2022 We introduce a hindsight-like off-policy strategy to speed up learning in HN-GFN.\u2022 Experiments verify that our MOBO algorithm based on HN-GFN can find high-quality Pareto front more efficiently compared to state-of-the-art baselines."
related_work_iclr_2023_1 = "Related Work: Molecular optimization. Recently, molecular optimization has been approached with a wide variety of computational methods, which can be mainly grouped into three categories: 1) Latent space optimization (LSO) methods perform the optimization over the low-dimensional continuous latent space learned by generative models such as variational autoencoders (VAEs) (G\u00f3mez-Bombarelli et al., 2018; Maus et al., 2022) . These methods require the latent representations to be discriminative, but the training of the generative model is decoupled from the optimization objectives, imposing challenges for optimization (Tripp et al., 2020) . Instead of navigating the latent space, combinatorial optimization methods search for the desired molecular structures directly in the explicit discrete space with 2) evolutionary algorithms (Jensen, 2019) and 3) deep neural networks to guide the searching (You et al., 2018) . However, most prior methods only focus on optimizing a single property, from non-biological properties such as drug-likeliness (QED) (Bickerton et al., 2012) and synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009) , to biological properties that measure the binding energy to a protein target (Bengio et al., 2021a) . Despite the above advances, multi-objective molecular optimization has recently received wide attention (Jin et al., 2020b; Xie et al., 2021; Fu et al., 2022) . For example, MARS (Xie et al., 2021) employs Markov chain Monte Carlo (MCMC) sampling to find novel molecules satisfying several properties. However, most approaches require a notoriously large number of oracle calls to evaluate molecules on-the-fly (Jin et al., 2020b; Xie et al., 2021) . In contrast, we tackle this problem in a sample-efficient manner.GFlowNet. GFlowNets (Bengio et al., 2021a) aim to sample composite objects proportionally to a reward function, instead of maximizing it as usual in RL (Sutton & Barto, 2018) . GFlowNets are related to the MCMC methods due to the same objective, while amortizing the high cost of sampling (mixing between modes) over training a generative model (Zhang et al., 2022) . GFlowNets have made impressive progress in various applications, such as active learning (Jain et al., 2022) , discrete probabilistic modeling (Zhang et al., 2022) , and Bayesian structure learning (Deleu et al., 2022) . For a thorough discussion and mathematical treatment, we refer the readers to Bengio et al. (2021a; b) Bayesian Optimization for discrete spaces. While the application of BO in continuous domains has proliferated during the last decade, effort in applying it to discrete spaces is lacking. It is much more challenging to construct surrogate models and optimize acquisition functions in discrete spaces, compared to continuous spaces. One common approach is to define GPs with discrete kernels (Moss et al., 2020) and solve the acquisition function optimization problem with evolutionary algorithms (Kandasamy et al., 2018) . Moreover, AmortizedBO (Swersky et al., 2020) proposes to augment the evolutionary algorithms with RL.Multi-objective Bayesian Optimization. BO has been widely used in MOO problems for efficiently optimizing multiple competing black-box functions. Most popular approaches are based on hypervolume improvement (Daulton et al., 2020) , random scalarizations (Knowles, 2006; Paria et al., 2020) , and entropy search (Hern\u00e1ndez-Lobato et al., 2016) . While there have been several approaches that take parallel evaluations (Bradford et al., 2018; Konakovic Lukovic et al., 2020) and diversity (Konakovic Lukovic et al., 2020) into account, they are limited to continuous domains."
method_iclr_2023_1_1 = "Method: We address the problem of searching over a discrete chemical space X to find molecular graphs x \u2208 X that maximize a vector-valued objective f(x) = f 1 (x), f 2 (x), . . . , f M (x) : X \u2192 R M, where f m is a black-box function (also known as the oracle) evaluating a certain property of molecules. Practically, realistic oracles are extremely expensive to evaluate with either high-fidelity simulations or wet-lab experiments. We thus propose to perform optimization in as few oracle evaluations as possible, since the sample efficiency is paramount in such a scenario.There is typically no single optimal solution to the MOO problem, as different objectives may contradict each other. Consequently, the goal is to recover the Pareto front -the set of Pareto optimal solutions which cannot be improved in any one objective without deteriorating another (Ehrgott, 2005; Miettinen, 2012) . In the context of maximization, a solution f (x) is said to Pareto dominatesanother solution f (x \u2032 ) iff f m (x) \u2265 f m (x \u2032 ) \u2200m = 1, . . . , M and \u2203m \u2032 such that f m \u2032 (x) > f m \u2032 (x \u2032 ),and we denote f (x) \u227b f (x \u2032 ). A solution f (x * ) is Pareto optimal if not Pareto dominated by any solution. The Pareto front can be written asP * = {f (x * ) : {f (x) : f (x) \u227b f (x * ) } = \u2205}.The quality of a finite approximate Pareto front P is commonly measured by hypervolume (HV) (Zitzler & Thiele, 1999) -the M-dimensional Lebesgue measure \u03bb M of the space dominated by P and bounded from below by a given reference point r \u2208 R M :HV (P, r) = \u03bb M (\u222a |P| i=1 [r, y i ]), where [r, y i ] denotes the hyper-rectangle bounded by r and y i = f (x i ).Bayesian optimization (BO) (Shahriari et al., 2015) provides a model-based iterative framework for sample-efficient black-box optimization. Given an observed dataset D, BO relies on a Bayesian surrogate model M to estimate a posterior distribution over the true oracle evaluations. Equipped with the surrogate model, an acquisition function a : X \u2192 R is induced to assign the utility values to candidate objects for deciding which to next evaluate the oracle. Compared with the costly oracle, the cheap-to-evaluate acquisition function can be efficiently optimized. We consider the scenario where the oracle is given an evaluation budget of N rounds with fixed batches of size b.To be precise, we have access to a random initial datasetD 0 = {(x 0 i , y 0 i )} n i=1, where y 0 i = f (x 0 i ) is true oracle evaluation. In each round i \u2208 {1, . . . , N }, the acquisition function is maximized to yield a batch of candidates B i = {x i j } b j=1 to be evaluated in parallel on the oracle y i j = f (x i j ). The observed dataset D i-1 is then augmented for the next round:D i = D i-1 \u222a {(x i 1 , y i j )} b j=1 ."
method_iclr_2023_1_2 = "In this section, we present the proposed MOBO algorithm based on hypernetwork-based GFlowNet (HN-GFN), shown in Figure 1 . Our key idea is to extend GFlowNets as the acquisition function optimizer for MOBO, with the objective to sample a diverse batch of candidates from the approximate Pareto front. To begin, we introduce GFlowNets in the context of molecule design, then describe how GFlowNet can be biased by a preference-conditioned hypernetwork to sample molecules according to various trade-offs between objectives. Next, we propose a hindsight-like off-policy strategy to speed up learning in HN-GFN. Lastly, we introduce the evidential surrogate model.GFlowNets (Bengio et al., 2021a) seek to learn a stochastic policy \u03c0 for sequentially constructing discrete objects x \u2208 X with a probability \u03c0(x) \u221d R(x), where X is a compositional space and R : X \u2192 R \u22650 is a non-negative reward function. The generation process of object x \u2208 X can be represented by a sequence of discrete actions a \u2208 A that incrementally modify a partially constructed object, which is denoted as state s \u2208 S. Let generation process begin at a special initial state s 0 and terminate with a special action indicating that the object is complete (s = x \u2208 X ), the construction of an object x can be defined as a complete trajectory \u03c4 = (s0 \u2192 s 1 \u2192 \u2022 \u2022 \u2022 \u2192 s n = x).Following fragment-based molecule design (Bengio et al., 2021a; Xie et al., 2021) , we first define a vocabulary of building blocks (molecule fragments), then generate molecular graphs by sequentially attaching a fragment to an atom of the partially constructed molecules. There are multiple action sequences leading to the same state, and no fragment deleting actions, the space of possible action sequences can thus be denoted by a directed acyclic graph (DAG) G = (S, E), where the edges in E are transitions s \u2192 s \u2032 from one state to another. To learn the aforementioned desired policy, Bengio et al. (2021a) propose to see the DAG structure as a flow network.Markovian flows. Bengio et al. (2021b) first define a trajectory flow F : T \u2192 R \u22650 on the set of all complete trajectories T to measure the unnormalized density. The edge flow and state flow can then be defined as F (s \u2192 s \u2032 ) = s\u2192s \u2032 \u2208\u03c4 F (\u03c4 ) and F (s) = s\u2208\u03c4 F (\u03c4 ), respectively. The trajectory flow F determines a probability measure P (\u03c4 ) = (\u03c4 ) . If flow F is Markovian, the forward transition probabilities P F can be computed as P F (s \u2032 |s) = F (s\u2192s \u2032 ) F (s) .F (\u03c4 ) \u03c4 \u2208T FFlow matching objective. A flow is consistent if the following flow consistency equation is satisfied \u2200s \u2208 S:EQUATIONwhere P a G (s) is a set of parents of s in G. As proved in Bengio et al. (2021a) , if the flow consistency equation is satisfied with R(s) = 0 for non-terminal state s and F (x) = R(x) \u2265 0 for terminal state x, a policy \u03c0 defined by the forward transition probability \u03c0(s \u2032 |s) = P F (s \u2032 |s) samples object x with a probability \u03c0(x) \u221d R(x). GFlowNets propose to approximate the edge flow F (s \u2192 s \u2032 ) using a neural network F \u03b8 (s, s \u2032 ) with enough capacity, such that the flow consistency equation is respected at convergence. To achieve this, Bengio et al. (2021a) define a temporal difference-like (Sutton & Barto, 2018) learning objective, called flow-matching (FM): Bengio et al. (2021a) prove that we can use any exploratory policy \u03c0 with full support to sample training trajectories and obtain the consistent flow F \u03b8 (s, s \u2032 ) by minimizing the FM objective. Consequently, a policy defined by this approximate flow \u03c0 \u03b8 (sL FM (s, R; \u03b8) = log s \u2032 \u2208P a G (s) F \u03b8 (s \u2032 , s) R(s) + s \u2032\u2032 :s\u2208P a G (s \u2032\u2032 ) F \u03b8 (s, s \u2032\u2032 ) 2 (2)\u2032 |s) = P F \u03b8 (s \u2032 |s) = F \u03b8 (s\u2192s \u2032 ) F \u03b8 (s)can also sample objects x with a probability \u03c0 \u03b8 (x) proportionally to reward R(x). Practically, the training trajectories are sampled from an exploratory policy which is a mixture between P F \u03b8 and a uniform distribution over allowed actions (Bengio et al., 2021a) .Our proposed HN-GFN aims at sampling a diverse batch of candidates from the approximate Pareto front with a unified model. A common approach to MOO is to decompose it into a set of scalar optimization problems with different scalarization functions and apply standard single-objective optimization methods to gradually approximate the Pareto front (Knowles, 2006; Zhang & Li, 2007) . We here consider convex combinations (weighted sum) of the objectives. Let \u03bb = (\u03bb i , \u2022 \u2022 \u2022 , \u03bb M ) \u2208 S M be a preference vector defining the trade-off between the competing properties, whereS M = {\u03bb \u2208 R m | i \u03bb i = 1, \u03bb i \u2265 0} is the M -1 simplex. Then the scalarization function can be formulated as s \u03bb (x) = i \u03bb i f i (x).To support parallel evaluations in BO, one can obtain candidates according to different scalarizations (Daulton et al., 2020) . Practically, this approach hardly scales efficiently with the number of objectives for discrete problems. Taking GFlowNet as an example, we need to train multiple GFlowNets independently for each choice of the reward function R \u03bb (x) = s \u03bb (x) to cover the objective space:EQUATIONOur key motivation is to design a unified GFlowNet to sample candidates according to different reward functions, even ones not seen during training. Instead of defining the reward function with a fixed preference vector \u03bb, we propose to train a preference-conditioned GFlowNet on a distribution of reward functions R \u03bb , where the preference vector \u03bb is sampled from a simplex S M :EQUATIONNote that the preliminary concept of conditional GFlowNet was originally introduced in Bengio et al. (2021b) . We study and instantiate this concept, aiming to facilitate MOO in the context of molecule design.Remarks. Our proposed optimization scheme of training a single model to fit a family of loss functions fits into the framework of YOTO (Dosovitskiy & Djolonga, 2019). As proved in Dosovitskiy & Djolonga (2019) , assuming an infinite model capacity, the proposed optimization scheme (Eq. 4) is as powerful as the original one (Eq. 3), since the solutions to both loss functions coincide. Nevertheless, the assumption of infinite capacity is extremely strict and hardly holds, so how to design the conditioning mechanism in practice becomes crucial.We propose to condition the GFlowNets on the preference vectors via hypernetworks (Ha et al., 2016) . Hypernetworks are deep networks that generate the weights of a target network based on inputs. In vanilla GFlowNets, the flow predictor F \u03b8 is parameterized with the MPNN (Gilmer et al., 2017) over the graph of molecular fragments, with two prediction heads approximating F (s, s \u2032 ) and F (s) based on the node and graph representations respectively. These two heads are parameterized with multi-layer perceptrons (MLPs).One can view the training of HN-GFN as learning an agent to perform multiple policies that correspond to different goals (reward functions R) defined in the same environment (state space S and action space A). Therefore, we propose to only condition the weights of prediction heads \u03b8 pred with hypernetworks, while sharing the weights of MPNN \u03b8 mpnn , leading to more generalizable state representations. More precisely, a hypernetwork h(\u2022; \u03d5) takes as inputs the preference vector \u03bb to output the weights \u03b8 pred = h(\u03bb; \u03d5) of prediction heads in the flow predictor F \u03b8 . For brevity, we write \u03b8 = (\u03b8 mpnn , \u03b8 pred ). Following Navon et al. ( 2021), we parametrize h using a MLP with multiple heads, each generating weights for different layers of the target network.Training. At each iteration, we first randomly sample a new preference vector \u03bb from a Dirichlet distribution Dir(\u03b1). Then the HN-GFN is trained in a usual manner with the reward function set as R \u03bb (x) = a(\u00b5(s \u03bb (x)), \u03c3(s \u03bb (x)); M), where \u00b5 and \u03c3 are posterior mean and standard deviation estimated by M.At each round i, we use the trained HN-GFN to sample a diverse batch of b candidates. Let \u039b i be the set of l target preference vectors \u03bb i target . We sample b l molecules per \u03bb i target \u2208 \u039b i and evaluate them on the oracle in parallel. In practice, we simply sample \u03bb i target from Dir(\u03b1), but it is worth noting that this prior distribution can also be defined adaptively based on the trade-off of interest. As the number of objectives increases, we choose a larger l to cover the objective space.Resorting to the conditioning mechanism, HN-GFN can learn a family of policies to achieve various goals, i.e., one can treat sampling high-reward molecules for a particular preference vector as a separate goal. As verified empirically in Jain et al. (2022) , since the FM objective is off-policy and offline, we can use offline trajectories to train the target policy for better exploration, so long as the assumption of full support holds. Our key insight is that each policy can learn from the valuable experience (high-reward molecules) of other similar policies.To achieve this, inspired by Hindsight Experience Replay (Andrychowicz et al., 2017) in RL, we propose to share high-performing molecules among policies by re-examining them with different preference vectors. Because there are infinite possible preference vectors, here we only focus on \u039b i , which are based on to sample candidates at round i, and build a replay buffer for each \u03bb i target \u2208 \u039b i . After sampling some trajectories during training, we store in the replay buffers the complete object x with the reward R \u03bb i target (x). Algorithm 2 describes the training procedure for HN-GFN with the proposed hindsight-like strategy. At each iteration, we first sample a preference vector from a mixture between Dir(\u03b1) and a uniform distribution over \u039b i : (1 -\u03b3)Dir(\u03b1) + \u03b3Uniform. If \u039b is chosen, we construct half of the training batch with offline trajectories from the corresponding replay buffer of molecules encountered with the highest rewards. Otherwise, we incorporate offline trajectories from the current observed dataset D i instead to ensure that HN-GFN samples correctly in the vicinity of the observed Pareto set. While GPs are well-established in continuous spaces, they scale poorly with the number of observations and do not perform well in discrete spaces (Swersky et al., 2020) . There has been significant work in efficiently training non-Bayesian neural networks to estimate the uncertainty (Gal & Ghahramani, 2016) . In this work, we use evidential deep learning (Amini et al., 2020) to explicitly learn the epistemic uncertainty. Compared with the widely used MC Dropout (Gal & Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017) , evidential deep learning presents the advantages of faster inference speed and superior calibrated uncertainty (Soleimany et al., 2021) . As for the acquisition function, we use Upper Confidence Bound (Srinivas et al., 2010) to incorporate epistemic uncertainty. To be precise, the objectives are modeled with a single multi-task network and the acquisition function is applied to the scalarization. See Appendix C.3 for more discussion."
method_iclr_2023_1 = method_iclr_2023_1_1 + method_iclr_2023_1_2
experiments_iclr_2023_1 = 'Experiments and Results: We first verify that HN-GFN has adequate capacity to generalize over preference vectors in a synthetic scenario. Next, we evaluate the effectiveness of the proposed MOBO algorithm based on HN-GFN in practical scenarios, which are more in line with real-world molecular optimization. Implementation details and additional results are provided in the Appendix.Here, our goal is to demonstrate that we can leverage the HN-GFN to sample molecules with preference-conditioned property distributions. The HN-GFN is used as a stand-alone optimizer outside of MOBO to directly optimize the scalarizations of oracle scores. As the oracle cannot be called as many times as necessary practically, we refer to this scenario as a synthetic scenario. To better visualize the trend of the property distribution of the sampled molecules as a function of the preference vector, we only consider two objectives: inhibition scores against glycogen synthase kinase-3 beta (GNK3\u03b2) and c-Jun N-terminal kinase-3 (JNK3) (Li et al., 2018; Jin et al., 2020b) .Compared methods. We compare HN-GFN against the following methods. Preference-specific GFlowNet is a vanilla GFlowNet trained independently for a particular preference vector. Note that the preference-specific GFlowNet is treated as "gold standard" rather than the baseline, as it is trained and evaluated using the same preference vector. Concat-GFN and FiLM-GFN are two variations of the conditional GFlowNet based on FiLM (Perez et al., 2018) and concatenation, respectively. MOEA/D (Zhang & Li, 2007) and NSGA-III (Deb & Jain, 2013) are two multi-objective evolutionary algorithms that also incorporate preference information. We perform evolutionary algorithms over the 32-dim latent space learned by HierVAE (Jin et al., 2020a) , which gives better optimization performance than JT-VAE (Jin et al., 2018) .Metrics. All the above methods are evaluated over the same set of 5 evenly spaced preference vectors. For each GFlowNet-based method, we sample 1000 molecules per preference vector as the solutions. We compare the aforementioned methods on the following metrics: Hypervolume indicator (HV) measures the volume of the space dominated by the Pareto front of the solutions and bounded from below by the preference point (0, 0). Diversity (Div) is the average pairwise Tanimoto distance over Morgan fingerprints. Correlation (Cor) is the Spearman\'s rank correlation coefficient between the probability of sampling molecules from an external test set under the GFlowNet and their respective rewards in the logarithmic domain (Nica et al., 2022) . See more details in Appendix B.1.2. In a nutshell, HV and Div measure the quality of the solutions, while Cor measures how well the trained model is aligned with the given preference vector.Experimental results. As shown in Table 1 , HN-GFN outperforms the baselines and achieves competitive performance to the preference-specific GFlowNets (gold standard) on all the metrics. Compared to the GFlowNet-based methods, the evolutionary algorithms (MOEA/D and NSGA-III) fail to find high-scoring molecules, especially the MOEA/D. HN-GFN outperforms Concat-GFN and FiLM-GFN in terms of HV and Cor, implying the superiority of the well-designed hypernetwork-based conditioning mechanism. The comparable performance of HN-GFN and preference-specific GFlowNets illustrates that HN-GFN can generalize over preference vectors. Therefore, the unified HN-GFN provides a significantly efficient way to explore various trade-offs between objectives. In Figure 2 (Left), we visualize the trend of the empirical distribution of JNK3 as the preference weight increases. Intuitively, HN-GFN and preference-specific GFlowNets show consistent trends: the larger the preference weight, the higher the average score.Next, we evaluate the effectiveness of HN-GFN as an acquisition function optimizer within MOBO in the practical scenarios, where there is a limited evaluation budget for oracle. We consider the following objective combinations of varying size:\u2022 GNK3\u03b2+JNK3: Jointly inhibiting Alzheimer-related targets GNK3\u03b2 and JNK3.\u2022 GNK3\u03b2+JNK3+QED+SA: Jointly inhibiting GNK3\u03b2 and JNK3 while being drug-like and easy-to-synthesize.We rescale the SA score such that all the above properties have a range of [0,1] and higher is better. For both combinations, we consider starting with |D 0 | = 200 random molecules and further querying the oracle N = 8 rounds with batch size b = 100.Baselines. We compare HN-GFN with the following methods (from each category mentioned in section 2) as the acquisition function optimizer: HierVAE (Jin et al., 2020a) with qParEGO/qEHVI (Daulton et al., 2020) and LaMOO (Zhao et al., 2022) are LSO methods. GP-BO (Tripp et al., 2021) uses Graph GA (Jensen, 2019) to optimize the acquisition function defined based on a GP with Tanimoto kernel. MARS (Xie et al., 2021) applies MCMC sampling to optimize the acquisition function defined based on the same surrogate function as HN-GFN. Note that the RL-based P-MOCO (Xi Lin, 2022) is also implemented but fails to optimize the properties.Experimental results. Table 2 shows that HN-GFN achieves superior performance over the baselines in terms of HV and Div, especially trained with the hindsight-like off-policy strategy. Note that the Div is computed among the batch of 100 candidates per round, we omit this metric for LSO methods as they only support 160 rounds with batch size 5 due to memory constraint. Our HN-GFN w/ hindsight outperforms the best baselines MARS and GP-BO of the two objective combinations by a large margin (60.0% and 24.2% relative improvement) with respect to HV, respectively. The promising performance can be attributed to the ability of HN-GFN to sample a diverse batch of candidates from the approximate Pareto front. Another interesting observation, in the more challenging settings where four objectives are optimized, is that MARS generates diverse candidates via MCMC sampling but fails to find high-quality Pareto front, indicating that HN-GFN can find high-reward modes better than MARS. The computational costs are discussed in the Appendix B.4.Effect of the hindsight-like strategy. In the first round of MOBO, for each \u03bb target \u2208 \u039b we sample 100 molecules every 500 training steps and compute the average Top-20 reward over \u039b. In Figure 2 (Right), as we vary \u03b3 from 0 to 1, the hindsight-like strategy significantly boosts average rewards, demonstrating that sharing high-performing molecules among policies is effective for speeding up the training of HN-GFN. We choose \u03b3 = 0.2 for the desired trade-off between reward and generalization, see Appendix C.2 for a detailed explanation.Effect of \u03b1. Next, we study the effect of the prior distribution of preference vectors Dir(\u03b1). We consider the more challenging GNK3\u03b2+JNK3+QED+SA combination, where the difficulty of optimization varies widely for various properties. Table 3 shows that the distribution skewed toward harder properties results in better optimization performance.Effect of scalarization functions. In addition to the weighted sum (WS), we consider the Tchebycheff (Miettinen, 2012) that is also commonly used in MOO. Table 3 shows that Tchebycheff leads to a worse Pareto front compared to WS. We conjecture that the non-smooth reward landscapes induced by Tchebycheff are harder to optimize. '
conclusion_iclr_2023_1 = "Conclusion: We have introduced a MOBO algorithm for sample-efficient multi-objective molecular optimization. This algorithm leverages a hypernetwork-based GFlowNet (HN-GFN) to sample a diverse batch of candidates from the approximate Pareto front. In addition, we present a hindsight-like off-policy strategy to improve optimization performance. Our algorithm outperforms existing approaches on synthetic and practical scenarios. Future work includes extending this algorithm to other discrete optimization problems such as biological sequence design and neural architecture search.Correlation. Correlation (Cor) is the Spearman's rank correlation coefficient between the probability of sampling molecules from an external test set under the GFlowNet and their respective rewards in the logarithmic domain: Cor = Spearman's \u03c1 log(\u03c0(x)),log(R(x)) The external test set is obtained in two steps: First, we generate a random dataset containing 300K molecules uniformly based on the number of building blocks; Next, we sample the test sets with uniform property distribution corresponding to GSK3\u03b2 and JNK3, respectively, from the 300K molecules. The final test set contains 6062 molecules."

paper_text_iclr_2023_1 = f"{title_iclr_2023_1}\n {abstract_iclr_2023_1}\n {introduction_iclr_2023_1}\n {related_work_iclr_2023_1}\n{method_iclr_2023_1}\n{experiments_iclr_2023_1} \n {conclusion_iclr_2023_1}"

review_1_iclr_2023_1 = 'paper_summary: This work investigate the possibility of using GFlowNets to tackle multi-objective sampling problems. Specifically, hypernetwork-based GFlowNet is proposed for solving multi-objective Bayesian optimization problems. Some insights from reinforcement learning is also involved. The proposed algorithm is evaluated in molecule generation tasks.\n \
                        main_review: ## Clarity & Quality\nMost parts of the paper are clearly written. Some details are missing, though, for example $\\gamma$.\n\n## Novelty\nThe idea is actually not entirely, but has been long mentioned as "Pareto GFlowNet" in [GFlowNet Foundations](https://arxiv.org/pdf/2111.09266.pdf).\n\n## Reproducibility\nI cannot check the reproducibility as it seems the implementation is not provided.\n \
                        strength_weakness: strength_weakness: ## Strengths\n1. The methodology is well-formulated, with careful design for amortizing multi-objective problems and surrogate modeling.\n2. The off-policy property of GFlowNet is examined, including a mix of different training trajectory distribution, and the use of hindsight replay idea.\n\n## Weaknesses\n1. I would expect some illustration / analysis on at least toy tasks to show that the proposed method indeed learn the multi-objective behavior. E.g., when the conditioning preference is changed, the distribution of GFlowNet will change accordingly.\n2. The diversity should be measured for every fixed preference. If allowed different preference, there will be of course diverse samples generated by the GFlowNet. \n3. Is there any particular reason to model the preference conditioning with a hypernetwork?  Hypernetwork is not very stable in training. What\'s more, usually it is enough to directly augment the input with preference for the conditioning, which is also much easier. I would expect the authors to justify this choice with empirical evidence.\n4. Is there any particular reason to use evidental regression? According to my experience, sometimes it is not as reliable as more "traditional" methods like deep ensemble. Also, it seems HierVAE-based methods and GP-BO still uses GP, while the proposed method uses evidental regression. Is this an unfair comparison? I would expect at least some results about using HN-GFN with GP to justify this choice.\n\n### Minors\n- The experiments are limited to molecule generation. The multi-objective problems are much more than small graph generation. Other applications, such as protein, sequences, are also of great importance. \n- Is the set of target preference vectors $\\Lambda$ kept fixed? If not, how to update it?\n- In Fig.2 (left), why HN-GFN could be better than the "gold-standard" preference-specific GFlowNet? Do these two use the same GFlowNet architecture?\n- Fig.2 (right) shows the performance with $\\gamma=0, 0.2$, but it would be great to see a spectrum from 0 to 1. This is important for the choice of hyper parameter $\\gamma$, which seems not mentioned in the paper. Please correct me if I miss anything!\n \
                        questions: / \n\
                        limitations: / \n \
                        review_summary: The method of this paper is clean and straightforward, while this work still suffers from non-extensive experiments and insufficient ablation to justify some usage of the components. I understand that it is a tight schedule for the authors to rebuttal, but I would of course consider raising the score if some of the main concerns are addressed.\n \
                        score: 3 \n \
                        confidence: 4, \
                        novelty: 2, \
                        correctness: 3, '

review_2_iclr_2023_1 = 'paper_summary: Practical molecule generation involve optimization of multiple objectives simultaneously. These objectives are often expensive to evaluate making sample-efficiency key. The paper proposes a multi-objective Bayesian optimization method leveraging GFlowNets for optimizing the acquisition function. GFlowNets learn stochastic policies to generate discrete objects proportionally to their rewards, resulting in diverse candidates. The authors consider a preference-based decomposition of the MOO problem. The paper proposes a hypernetwork-based parameterization for conditioning on preferences. The authors also propose a hindsight-experience replay based strategy for leveraging offline data during learning. The authors use an evidential regressor as the surrogate model within the multi-objective Bayesian optimization context. The authors then present results on a molecule generation task with 4 objectives. \n \
                        main_review: **Clarity**\n\nFor the most part the discussion in the paper is clear. However, there are several places where prior work is incorrectly cited / missed out completely. In addition to the preference-conditional GFlowNet mentioned in the previous section, on line 4 on page 4, the authors cite Daulton et al. 2020 for the basic terminology of MOO instead of classic work in MOO [1,2]. On the second line in the paragraph before equation 3 the authors cite Daulton et al 2020 again for  "To support parallel evaluations in BO, one can obtain candidates according to different scalarizations", however, Daulton et al. 2020 does not consider scalarization at all. In fact it is other work [3] which establishes such approaches. Aside from this, there are aspects of the method which are also not clear. For example, the authors mention they use UCB as the acquisition function, but do not discuss how the UCB is used in this multi-objective setting - is the UCB applied to each objective individually or to the scalarization?\n\n**Quality and Novelty** \n\nWhile some of the underlying ideas are not particularly novel - preference conditioning was introduced in [4] and GFlowNets in the context of BO was studied in [5] - the hypernetwork-based conditioning and hindsight experience replay are novel in the context of GFlowNets. However, as discussed in the previous section the empirical evidence is not substantial enough. \n\n**Reproducibility**\n\nThe authors do not provide code with the submission. The appendix does contain some relevant hyperparameters but some implementation details are not discussed at all. For example, no details are discussed about the training of the surrogate model - for instance whether the surrogate for each property is trained independently or a single multi-task model is trained, as well as other training details. The authors also do not mention other important hyperparameters like the UCB parameter (controlling exploration and exploitation). \n\n\n[1] Multicriteria optimization, Ehrgott, 2005\n\n[2] Nonlinear Multiobjective Optimization, Miettinen, 2012\n\n[3] A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations\n\n[4] GFlowNet Foundations\n\n[5] Biological Sequence Design with GFlowNets \n \
                        strength_weakness: **Strengths**\n\n- The paper tackles an important and challenging problem of multi-objective optimization in the context of molecule generation. As shown in previous work, using GFlowNets for optimizing the acquisition function results in diverse candidates and sample-efficient optimization\n- The hypernetwork-based approach is an interesting way to implement conditioning, in contrast to FiLM based approaches. \n\n**Weaknesses**\n- The paper overall is not very clearly written (I discuss this in more detail in the next section)\n- The paper uses the preference-conditional GFlowNet formulation originally proposed in [1], but does not cite the paper where the preference-conditional GFlowNet is introduced. There are also other other inconsistent citations which I discuss in the next section,\n- Multi-Objective REINFORCE [2] is very closely related to the proposed HN-GFN approach, differing only in the learning objective, but is not discussed / included as a baseline. Additionally the authors also do not include recent approaches such as LaMOO [3] in the baselines. \n- Aside from the baselines, the experiments seem somewhat limited. While the method enjoys superior performance in the task studied in the paper, it is not clear how well it generalizes to different settings and even different rewards for instance. The authors also provide only limited ablations to investigate the method. For instance it is not clear how the set of preference vectors is selected and what is the effect of the distribution of preference vectors used in training. \n- Minor: The authors use evidential regression for the surrogate model claiming "evidential deep learning presents the\nadvantages of faster inference speed and superior calibrated uncertainty" however, recent work [4] has established that such approaches can be arbitrarily miscalibrated. \n\n\n[1]  - GFlowNet Foundations\n\n[2] - Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization\n\n[3] - Multi-objective Optimization by Learning Space Partitions\n\n[4] - Pitfalls of Epistemic Uncertainty Quantification through Loss Minimisation \n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: In summary, while the paper presents an interesting GFlowNet-based approach to tackle multi-objective optimization, there are several major shortcomings in the paper in terms of the empirical analysis, baselines and framing of contributions. In the current state I lean towards rejection but encourage the authors to incorporate the feedback to improve the paper during the discussion. \n \
                        score: 6, \n \
                        confidence: 4 \n \
                        novelty: 2 \n \
                        correctness: 3 '

review_3_iclr_2023_1 = "paper_summary: The paper proposes a multi-objective Bayesian optimization approach for the molecules design problem. The proposed approach uses the hypernetwork-based GFlowNets as an acquisition function optimizer and uses a scalarization approach to combine the multiple objectives.\n \
                        main_review: The novelty is limited since the paper uses a combination of previous approaches. \n\nReproducibility is also concerning since the paper reports only three runs.\n \
                        strength_weakness: Strengths: \n+ The paper presents an important scientific application. \n+ The paper presents some promising experimental results, though I have some reservations about the robustness of the results\n+ The paper is easy to follow \n+ The paper addresses the multi-objective problem, which is relatively less studied in the context of molecules, but it is worth noting that it has been recently extensively studied in the general Bayesian optimization problem\n\nWeaknesses\n\n+ The proposed technique is a direct combination of existing techniques with no new substantial addition. Therefore, the technical contribution and novelty are weak\n+ The paper uses the following statement \u201cWe assume that the oracle can be called as many times as necessary.\u201d In Expensive settings, this is not usually true. \n+ The paper does not provide any time complexity analysis of the training. This is problematic because existing approaches are actually very fast, while gflownet is certainly much more expensive, so a time comparison and a discussion about complexity and tradeoffs are important.\n+ The paper discusses the sampling of the scalars extensively, but later in experiments, it is mentioned that 5 evenly-spaced preference vectors are used. It is not clear how this works exactly. \n+ State of the art \n    - State-of-the-art methods in molecular optimization are not stated or compared to.  The following is considered SOTA work and covers a wide range of relevant methods and benchmarks that should be discussed for fairness. [1]\n    - Most multi-objective BO papers are not mentioned nor compared to. In batch optimization the most efficient and high-performing methods are [2,3,4]. It is also misleading to state that no previous paper discussed diversity while [2] is a diversity-oriented method, and several single objective batch BO papers discussed diversity. \n    - The paper uses a scalarization technique where scalars are sampled from a distribution. This technique was previously proposed and used [5]. \n    - The Preference-based problem has been studied beyond the discussion that was mentioned in the paper. It is concerning to completely ignore principled existing work and claim it as a novelty. The following are some of the approaches, to name a few [6,7,8]\n\n+ The experimental setup is weak and surprising:\n- the paper states in the beginning that the evaluation was on several synthetic experiments and real-world experiments, but there are only two experiments. \n    - The paper uses three runs only to report the mean and standard error. BO papers report AT LEAST 10 runs usually, and most recent papers report 50 to 100 runs. I don\u2019t think 3 runs can provide any statistical significance or deliver any conclusions about performance. In fact, even expensive deep learning models are usually tested with a higher number of runs. \n    - The diversity is not reported for some of the algorithms. If diversity is a metric applied to the Pareto front, why can\u2019t it be applied to some of the approaches? \n    - The paper reports results for batch size 100 only. Batch BO papers usually evaluate several batch sizes. \n    - There is a total absence of many relevant baselines from molecular optimization, Bayesian optimization, and preference-based optimization. The paper is mainly experimental since the technical novelty is weak. Therefore, it needs to present a thorough experimental evaluation. \n\n[1] Maus, Natalie, et al. Local Latent Space Bayesian Optimization over Structured Inputs. arXiv preprint arXiv:2201.11872 (2022).\n\n[2] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems 33 (2020): 17708-17720.\n\n[3] Eric Bradford, Artur M Schweidtmann, and Alexei Lapkin. Efficient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of global optimization, 71(2):407\u2013438, 2018.\n\n[4] Syrine Belakaria and Aryan Deshwal. Uncertainty-aware search framework for multi-objective bayesian optimization. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\n[5] Paria, Biswajit, Kirthevasan Kandasamy, and Barnab\u00e1s P\u00f3czos. A flexible framework for multi-objective bayesian optimization using random scalarizations. Uncertainty in Artificial Intelligence. PMLR, 2020.\n\n[6] Abdolshah M, Shilton A, Rana S, Gupta S, Venkatesh S. Multi-objective Bayesian optimization with preferences over objectives. Advances in neural information processing systems. 2019;32.\n\n[7] Taylor, Kendall, et al. Bayesian preference learning for interactive multi-objective optimization. Proceedings of the Genetic and Evolutionary Computation Conference. 2021.\n\n[8] Lin, Zhiyuan Jerry, et al. Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes. International Conference on Artificial Intelligence and Statistics. PMLR, 2022. \n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: The paper addresses an important problem however, the novelty and experimental setup are limited. \n \
                        score: 5 \n \
                        confidence: 3 \n \
                        novelty: 2 \n  \
                        correctness: 2 "

review_4_iclr_2023_1 = 'paper_summary: This paper proposes a method for multi-objective Bayesian optimization for molecular optimization. The proposed method uses GFlowNets to optimize the acquisition function in BO, and uses a hypernetwork-based method to incorporate the preference vector into GFlowNets such that a diverse set of points can be sampled from the Pareto front. \n \
                        main_review: Clarify: The paper is well written in general, but some of the algorithmic details can be better explained, as I discussed above.\n\nQuality: The algorithmic design and the experiments are of high quality.\n\nNovelty: The problem of multi-objective BO for molecular optimization is intuitive and hence not novel, but the use of the hypernetwork to condition on different preference vectors in GFlowNets is novel as far as I know.\n\nReproducibility: Some experimental details are discussed, but the code is not uploaded. \n \
                        strength_weakness: Strengths:\n- The proposed method is intuitive and modifies GFlowNets in a reasonable way to facilitate multi-objective optimization.\n- The experiments are nicely done, and the experimental results look promising.\n- The paper is in general well written, and the proposed method is well motivated.\n\nWeaknesses:\n- I feel that some of the algorithmic details are not clearly explained, particularly the connection between Algorithm 1 (for training HN-GFN) and BO. For example, does the dataset $\\mathcal{D}$ correspond to the currently available observations from all previous iterations of BO? Does the reward function $R$ here correspond to the acquisition function calculated in BO? How is the set of target preference vectors built? More importantly, do you need to run Algorithm 1 after every iteration (or every batch) of BO? If yes, then the computational costs may become an issue and hence should discussed.\n- I think Section 4.3 needs to be revised to make it clearer, in the current form, it\'s not easy to understand.\n- Top paragraph of page 2: it\'s still unclear to me why limitation 1) makes "existing discrete molecular optimization methods" not applicable as "acquisition function optimizer". Can\'t you simply use those acquisition functions which directly take diversity into account? For example, if you use the GP-BUCB acquisition function from paper [1] below, to select an input in a batch, you can simply invoke an existing discrete molecular optimization method to maximize the acquisition function (whose GP posterior standard deviation is updated every time a new input is selected), which will naturally lead to a diverse set of inputs in a batch.       \n[1] Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization, JMLR 2014\n- Top paragraph of page 9: the number of rounds $N=8$ is in fact unusually small in BO, and the batch size $b=100$ is also unusually large for BO as well. Are these choices the common practice in molecular optimization using GFlowNet?\n- (minor) In the Related Work section, the previous works on multi-objective BO should also be discussed.\n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: The paper solves an important problem for molecular optimization using GFlowNets, and I don\'t have major concerns about the paper. The concerns I listed under "Weaknesses" are mostly regarding the writing of the paper. \n \
                        score: 8 \n \
                        confidence: 3 \n \
                        novelty: 4 \n \
                        correctness: 3 '


review_text_iclr_2023_1 = f"Review 1: {review_1_iclr_2023_1}\n Review 2: {review_2_iclr_2023_1}\n Review 3: {review_3_iclr_2023_1} \n Review 4: {review_4_iclr_2023_1}"


# Data for Paper 2 of ICLR-2023
title_iclr_2023_2 = (
    "Title: Scaling Up Probabilistic Circuits by Latent Variable Distillation"
)
abstract_iclr_2023_2 = "Abstract: Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD."
introduction_iclr_2023_2 = "Introduction: The development of tractable probabilistic models (TPMs) is an important task in machine learning: they allow various tractable probabilistic inference (e.g., computing marginal probabilities), enabling a wide range of down-stream applications such as lossless compression (Liu et al., 2022) and constrained/conditional generation (Peharz et al., 2020a) . Probabilistic circuit (PC) (Choi et al., 2020 ) is a unified framework for a wide range of families of TPMs, examples include bounded tree-width graphical models (Meila & Jordan, 2000) , And-Or search spaces (Marinescu & Dechter, 2005) , hidden Markov models (Rabiner & Juang, 1986) , Probabilistic Sentential Decision Diagrams (Kisa et al., 2014) and sum-product networks (Poon & Domingos, 2011 ). Yet, despite the tractability of PCs, scaling them up for generative modeling on large and high-dimensional vision/language dataset has been a key challenge.By leveraging the computation power of modern GPUs, recently developed PC learning frameworks (Peharz et al., 2020a; Molina et al., 2019; Dang et al., 2021) have made it possible to train PCs with over 100M parameters (e.g., Correia et al. (2022) ). Yet these computational breakthroughs are not leading to the expected large-scale learning breakthroughs: as we scale up PCs, their performance immediately plateaus (dashed curves in Fig. 1 ), even though their actual expressive power should increase monotonically with respect to the number of parameters. Such a phenomenon suggests that the existing optimizers fail to utilize the expressive power provided by large PCs. PCs can be viewed as latent variable models with a deep hierarchy of latent variables. As we scale them up, size of their latent space increases significantly, rendering the landscale of the marginal likelihood over observed variables highly complex. We propose to ease this optimization bottleneck by latent variable distillation (LVD): we provide extra supervision to PC optimizers by leveraging less-tractable yet more expressive deep generative models to induce semantics-aware assignments to the latent variables of PCs, in addition to the observed variables.The LVD pipeline consists of two major components: (i) inducing assignments to a subset of (or all) latent variables in a PC by information obtained from deep generative models and (ii) estimating PC parameters given the latent variable assignments. For (i), we focus on a clustering-based approach throughout this paper: we cluster training examples based on their neural embeddings and assign the same values to latent variables for examples in the same cluster; yet, we note that there is no constraint on how we should assign values to latent variables and the methodology may be engineered depending on the nature of the dataset and the architecture of PC and deep generative model. For (ii), to leverage the supervision provided by the latent variable assignments obtained in (i), instead of directly optimizing the maximum-likelihood estimation objective for PC training, we estimate PC parameters by optimizing the its lower-bound shown on the right-hand side:EQUATIONwhere {x (i) } N i=1 is the training set and z (i) is the induced assignments to the latent variables for x (i) . After LVD, we continue to finetune PC on the training examples to optimize the actual MLE objective, i.e., i log p(x (i) ).As shown in Figure 1 , with LVD, PCs successfully escape the plateau: their performance improves progressively as the number of parameters increases. Throughout the paper, we highlight two key advantages of LVD: first, it makes much better use of the extra capacity provided by large PCs; second, by leveraging the supervision from distilled LV assignments, we can significantly speed up the training pipeline, opening up possibilities to further scale up PCs.We start by presenting a simple example where we apply LVD on hidden Markov models to improve their performance on language modeling benchmarks (Sec. 2). Then we introduce the basics for PCs (Sec. 3.1) and present the general framework of LVD for PCs (Sec. 3.2). The general framework is then elaborated in further details, focusing on techniques to speed up the training pipeline (Sec. 4). In Section 5, we demonstrate how this general algorithm specializes to train patch-based PCs for image modeling. Empirical results show that LVD outperforms SoTA TPM baselines by a large margin on challenging image modeling tasks. Besides, PCs with LVD also achieve competitive results against various widely-used deep generative models, including flow-based models (Kingma & Dhariwal, 2018; Dinh et al., 2016) and variational autoencoders (Maal\u00f8e et al., 2019) (Sec. 6 )."
related_work_iclr_2023_2 = "Related Work: There has been various recent endeavors to improve the performance of PCs on modeling complex and high-dimensional datasets. An extensively-explored direction is to construct or learn the structure of PCs that is tailored to the target dataset. For example, Gens & Pedro (2013) ; Dang et al. (2022) seek to progressively improve the PC structure during the optimization process. Many other work aim to construct good PC structures given the dataset in one shot, and then move on to parameter optimization (Rahman et al., 2014; Adel et al., 2015) . Model agnostic PC structures such as RAT-SPN (Peharz et al., 2020b) and extremely randomized PCs (XPCs) (Di Mauro et al., 2021) are also shown effective in various density estimation benchmarks. There are also papers that focus exclusively on scaling up a particular TPM. For example, scaling HMM (Chiu & Rush, 2020 ) uses techniques such as learning blocked emission probabilities to boost the performance of HMMs.Another line of work seek to scale up PCs by learning hybrid models with neural networks (NNs). Specifically, Shao et al. (2022) leverages the expressive power of NNs to learn expressive yet tractable conditional distributions; HyperSPN (Shih et al., 2021) uses NN to regularize PC parameters, which prevents large PCs from overfitting. Such hybrid models are able to leverage the expressiveness of NNs at the cost of losing tractability on certain queries."
method_iclr_2023_2_0 = 'Method: In this section, we consider the task of language modeling by hidden Markov models (HMM) as an illustrating example for LVD. In particular, we demonstrate how we can use the BERT model (Devlin et al., 2019) to induce semantics-aware assignments to the latent variables of HMMs. Experiments on the WikiText-2 (Merity et al., 2016) dataset show that our approach effectively boosts the performance of HMMs compared to their counterpart trained with only random initialization.Dataset & Model. The WikiText-2 dataset consists of roughly 2 million tokens extracted from Wikipedia, with a vocabulary size of 33278. Following prior works on autoregressive language modeling (Radford et al., 2019) , we fix the size of the context window to be 32: that is, the HMM model will only be trained on subsequences of length 32 and whenever predicting the next token, the model is only conditioned on the previous 31 tokens. In particular, we adopt a non-homogeneous HMM model, that is, its transition and emission probabilities at each position share no parameters; (b) Pipeline for inferring values for one latent variable Z30. We feed token sequences to the BERT model to obtain contextualized embeddings for their suffixes X30X31X32; then we cluster all suffix embeddings into h clusters; here h = 3 is the number of hidden states and the value for Z30 is set to the cluster id. We repeat this procedure independently to infer values for all Zis. Figure 2a ) shows its representation as a graphical model, where X i s are the observed variables and Z i s are the latent variables. To facilitate training and evaluation, we pre-process the tokens from WikiText-2 by concatenating them into one giant token sequence and collect all subsequences of length 32 to construct the train, validation and test sets, respectively.X 1 Z 32 X 2 X 32 Z 2 Z 1 (a) GraphicalLatent Variable Distillation. Let D = {x (i) } i be the training set; Figure 2 shows an example on how to induce, for each training example x (i) , its corresponding assignment to the latent variable Z 30 .We first feed all training examples to the BERT model to compute the contextualized embeddings for their suffixes X 30 X 31 X 32 . We cluster all suffix embeddings into h clusters by the K-means algorithm (Lloyd, 1982) , where h is the number of hidden states; then, we set Z 30 to be the cluster id of their corresponding suffixes, that is, suffixes in the same cluster get the same latent variable value: the intuition is that if the BERT embeddings of some suffixes are close to each other then the suffixes should be relatively similar, suggesting that they should be "generated" by the same hidden state. We repeat this procedure for 32 times to infer the values for all Z i s. Now we obtain an "augmented" training set D aug = {(x (i) , z (i) )} i , where z (i) are the corresponding assignments to the latent variables Z; then, as suggested by Equation 1, we maximize the lower-bound i log p(x (i) , z (i) ) for the true MLE objective i log p(x (i) ). The parameters of the HMM that maximize i log p(x (i) , z (i) ), denoted by \u03b8 * , can be solved in closed-form. Finally, using \u03b8 * as a starting point, we finetune the HMM model via EM to maximize the true MLE objective i log p(x (i) ).Experiments. We apply LVD to HMMs with a varying number of hidden states h = 128, 256, 512, 750, 1024 and 1250; for comparison, we also train HMMs with random initialization. Please refer to Appx. C for details about training. The plot on the right of Figure 1 shows the test perplexity of HMMs (w/ and w/o LVD) on WikiText-2: as the number of parameters in HMM increases, the performance of the HMMs trained with random parameter initialization immediately plateaus, while the performance of the HMMs trained with LVD progressively improves, suggesting that LVD effectively exploits the express power of the larger models.'
method_iclr_2023_2_1 = '\n The previous section uses HMM as a specific TPM to elaborate key steps in LVD. In order to generalize LVD to broader TPMs, this section introduces Probabilistic Circuit (PC), which is a unifying framework for a large collection of tractable probabilistic models.PCs (Choi et al., 2020) are an umbrella term for a large variety of TPMs. Their syntax and semantics are defined as follows.Definition 1 (Probabilistic Circuits). A PC p(X) that encodes a distribution over variables X is defined by a parameterized directed acyclic computation graph (DAG) with a single root node n r . Every node in the DAG corresponds to a computational unit. Specifically, every leaf node is defined by an input unit and every inner node n represents either a sum or product unit that receives inputs from its children, termed in(n). Each PC unit n encodes a distribution p n :EQUATIONwhere f n (x) is a univariate probability distribution (e.g., Gaussian, Categorical) defined on a variable in X and \u03b8 c|n is the parameter corresponds to edge (n, c). For every sum unit n, we assume all its edge parameters {\u03b8 c|n } c\u2208in(n) are non-negative and sum up to one. Intuitively, a product unit encodes a factorized distribution over its inputs, and a sum unit models a weighted mixture of its children\'s distributions. A PC represents the distribution encoded by its root unit n r . We further assume w.l.o.g. that PCs alternate between sum and product layers before reaching an input layer.A key property that separates PCs from many other generative models is their tractability, i.e., the ability to answer various queries exactly while efficiently. Such queries include common ones like marginals and conditional probabilities as well as task-specific ones such as structured prediction (Shao et al., 2022) and variational inference (Shih & Ermon, 2020) . The tractability of PCs is governed by structural constraints on their DAG structure. For example, smoothness and decomposability together guarantee linear time (w.r.t. size of the PC) computation of arbitrary marginal probabilities. Definition 2 (Smoothness and Decomposability). Define the (variable) scope \u03d5(n) of a PC unit n as the set of variables defined by all its descendent input units. A PC is smooth if for every sum unit n, all its children have the same scope:\u2200c 1 , c 2 \u2208 in(n), \u03d5(c 1 ) = \u03d5(c 2 ). A PC is decomposable if the children of every product unit n have disjoint scopes: \u2200c 1 , c 2 \u2208 in(n)(c 1 \u0338 = c 2 ), \u03d5(c 1 ) \u2229 \u03d5(c 2 ) = \u2205.PCs can be viewed as latent variable models with discrete latent spaces (Peharz et al., 2016) . Specifically, since a sum unit in a PC can be viewed as a mixture over its input distributions, it can also be interpreted as a simple latent variable model z p(x|z)p(z), where z decides which input to choose from and the summation enumerates over all inputs. Figure 3 shows such an example, where the sum unit in Figure 3 (b) represents the mixture over Gaussians in Figure 3 (a) .In general, the latent space for large PCs is hierarchical and deeply nested; as we scale them up, we are in effect scaling up the size/complexity of their latent spaces, making it difficult for optimizers to find good local optima. To overcome such bottleneck, we generalize the idea presented in Section 2 and propose latent variable distillation (LVD). The key intuition for LVD is to provide extra supervision on the latent variables of PCs by leveraging existing deep generative models: given a PC p(X); we view it as a latent variable model z p(X, Z = z) over some set of latents Z and assume that for each training example x (i) , a deep generative model can always induce some semantics-aware assignment Z = z (i) ; then, instead of directly optimizing the MLE objective i log p(x (i) ), we can optimize its lower-bound i log p(x (i) , z (i) ), thus incorporating the guidance provided by the deep generative model. The LVD pipeline consists of three major steps, elaborated in the following:Step 1: Materializing Latent Variables. The first step of LVD is to materialize some/all latent variables in PCs. By materializing latent variables, we can obtain a new PC representing the joint distribution Pr(X, Z), where the latent variables Z are explicit and its marginal distribution Pr(X) corresponds to the original PC. Although we can assign every sum unit in a PC an unique LV, the semantics of such materialized LVs depend heavily on PC structure and parameters, Algorithm 1 Materializing a LV in a PC 1: Input: A PC p(X) and a variable scope W for some sum unit in p(X) 2: Output: An augmented PC p defined over {X, Z}, where Z is the materialized LV corresponding to W 3: S W \u2190 {n : n \u2208 p s.t. n is a product unit and \u03d5(n) = W} \u25b7 Created as an ordered set 4: for j = 1 to |S W | do 5:Let nj be the jth unit in S WAdd an input unit c over Zi with distribution pc(zi) = 1 zi = j, 0 otherwise as a new child of njn1 n2 n3 . . . c1 c2 c3 c4 Z =1 Z =2 Z =3 Z =4 n1 n2 n3 . . . c1 c2 c3 c4Figure 4 : Materializing LVs in a PC.which makes it extremely hard to obtain supervision. Instead, we choose to materialize LVs based on subsets of observed variables defined by a PC. That is, each materialized LV corresponds to all PC units with a particular variable scope (cf. Def. 2). For example, we can materialize the latent variable Z, which corresponds to the scope \u03d5(n i ) (\u2200i \u2208 [3]), to construct the PC in Figure 3 (c) that explicitly represents p(X, Z), whose marginal distribution p(X) corresponds to the PC in Figure 3 (b) . Algorithm 1 (Peharz et al., 2016) provides one general way to materialize latent variables in PCs, where Figure 4 shows an example where the four product units c 1 , . . . , c 4 are augmented with input units Z = 1, . . . , Z = 4, respectively.Continuing with our example in Figure 3 , note that after materialization, the sum unit representing p(X, Z) in Figure 3 (c) is no longer a latent variable distribution: each assignment to X, Z uniquely determines the input distribution to choose, where the other inputs give zero probability under this assignment; we say that this sum unit is deterministic (Darwiche, 2003) . Definition 3 (Determinism). Define supp(n) as the set of complete assignments x \u2208 val(X) such thatp n (x) > 0. A sum unit n is deterministic if its children have disjoint supports: \u2200c 1 , c 2 \u2208 in(n)(c 1 \u0338 = c 2 ), supp(c 1 ) \u2229 supp(c 2 ) = \u2205.Determinism characterizes whether a sum unit introduces latent variables: by materializing some sum units with the scope, we enforce them to become deterministic. Intuitively, more deterministic sum units in PCs implies smaller latent spaces, which implies easier optimization; in fact, if all sum units in a PC are deterministic then the MLE solution can be computed in closed-form (Kisa et al., 2014) . By materializing more latent variables, we make PCs "more deterministic", pushing the optimization procedure towards a closed-form estimation.Step 2: Inducing Latent Variable Assignments. Latent variable materialization itself cannot provide any extra supervision to the PC training pipeline; in addition, we also need to leverage some existing deep generative models to induce semantics-aware assignments for the materialized latent variables. Though there is no general guideline on how the assignments should be induced, we focus on a clustering-based approach throughout this paper. Recall from Section 2, where we cluster the suffix embeddings generated by the BERT model and for each training example, we assign the latents the cluster id that its suffixes belong to. Similarly, for image modeling, in Section 5, we will show how to induce latent variable assignments by clustering the embeddings for patches of images. The main take-away is that the method for inducing latent variable assignments should be engineered depending on the nature of the dataset and the architecture of PC and deep generative model.Step 3: PC Parameter Learning. Given a PC p(X; \u03b8) with parameters \u03b8 and a training set D = {x (i) }; in Step 1, by materializing some set of latent variables Z, we obtain an augmented PC p aug (X, Z; \u03b8) whose marginal distribution on X corresponds to p(X; \u03b8); in Step 2, by leveraging some deep generative model G, we obtain an augmented training set D aug = {(x (i) , z (i) )}. Note that since p aug and p share the same parameter space, we can optimize N i=1 log p aug (x (i) , z (i) ; \u03b8) as a lower-bound for N i=1 log p(x (i) ; \u03b8):N i=1 log p(x (i) ; \u03b8) = N i=1 log z p aug (x (i) , z; \u03b8) \u2265 N i=1 log p aug (x (i) , z (i) ; \u03b8);we denote the parameters for p aug after optimization by \u03b8 * . Finally, we initialize p with \u03b8 * and optimize the true MLE objective with respect to the original dataset D, N i=1 log p(x (i) ; \u03b8).Summary. Here we summarize the general pipeline for latent variable distillation. Assume that we are given: a PC p(X; \u03b8) over observed variables X with parameter \u03b8, a training set D = {x (i) } and a deep generative model G:1. Construct a PC p aug (X, Z; \u03b8) by materializing a subset of latent variables Z in p(X; \u03b8); note that p and p aug share the same parameter space.2. Use G to induce semantics-aware latent variable assignments z (i) for each training example x (i) ; denote the augmented dataset as D aug = {x (i) , z (i) }.3. Optimize the log-likelihood of p aug w/ respect to D aug , i.e., i log p aug (x (i) , z (i) ; \u03b8); denote the parameters for p aug after optimization as \u03b8 * . 4. Initialize p(X, \u03b8) with \u03b8 * and then optimize the log-likelihood of p w/ respect to the original dataset D, i.e., i log p(x (i) ; \u03b8).'
method_iclr_2023_2_2 = "\n Another major obstacle for scaling up PCs is training efficiency. Specifically, despite recently developed packages (Dang et al., 2021; Molina et al., 2019) and training pipelines (Peharz et al., 2020a ) that leverage the computation power of modern GPUs, training large PCs is still extremely time-consuming. For example, in our experiments, training a PC with \u223c500M parameters on CIFAR (using existing optimizers) would take around one GPU day to converge. With the efficient parameter learning algorithm detailed in the following, training such a PC takes around 10 GPU hours.The most computationally expensive part in LVD is to optimize the MLE lower bound (Eq. 1) with regard to the observed data and inferred LVs, which requires feeding all training samples through the whole PC. By exploiting the additional conditional independence assumptions introduced by the materialized LVs, we show that the computation cost of this optimization process can be significantly reduced. To gain some intuition, consider applying LVD to the PC in Figure 3 (c) with materialized LV Z. For a sample x whose latent assignment z is 1, since the Gaussian distributions p 2 and p 3 are independent with this sample, we only need to feed it to the input unit corresponds to p 1 in order to estimate its parameters. To formalize this efficient LVD algorithm, we start by introducing the conditional independence assumptions provided by the materialized LVs.. . . Lemma 1. For a PC p(X), denote W as the scope of some units in p. Assume the variable scope of every PC unit is either a subset of W or disjoint with W. Let Z be the LV corresponds to W created by Algorithm 1. Then variables W are conditional independent of X\\W given Z.Z 1 =1 Z 1 = M 1 . . . Z 2 =1 Z 2 = M 2 \u2022\u2022\u2022 \u2022\u2022\u2022Proof of the above lemma is provided in Appx. A.1. Take Figure 4 as an example. Define the scope of {n i } 3 i=1 and {c i } 4 i=1 as W and the corresponding LV as Z; denote the scope of the full PC as X. Lemma 1 implies that variables W and X\\W are conditional independent given Z.We consider a simple yet effective strategy for materializing LVs: the set of observed variables X is partitioned into k disjoint subsets {X i } k i=1 ; then for each X i , we use Algorithm 1 to construct a corresponding LV, termed Z i . As a direct corollary of Lemma 1, the joint probability over X and Z can be decomposed as follows:p(x, z) = p(z) k i=1 p(x i |z i ).The key to speed up LVD is the observation that the MLE lower bound objective (Eq. 1) can be factored into independent components following the decomposition of p(x, z): where D aug := {(x (l) , z (l) )} N l=1 is the training set augmented with LV assignments. According to Equation (3), optimize LL(p, D aug ) is equivalent to performing MLE on the factorized distributions separately. Specifically, we decompose the optimization process into the following independent steps: (i) for each cluster i and category j, optimizing PC parameters w.r.t. the distribution p(X i |Z i = j) using the subset of training samples whose LV z i is assigned to category j, and (ii) optimizing the sub-PC corresponds to p(z) using the set of all LV assignments. Consider the example PC shown in Figure 5 . The subset of PC surrounded by every blue box encodes the distribution labeled on its edge. To maximize LL(p, D aug ), we can separately train the sub-PCs correspond to the decomposed distributions, respectively. Compared to feeding training samples to the whole PC, the above procedure trains every latent-conditioned distribution p(X i |Z i = j) using only samples that have the corresponding LV assignment (i.e., z i = j), which significantly reduces computation cost.EQUATIONRecall from Section 3.2 that in the LVD pipeline, after training the PC parameters by maximizing LL(p, D aug ), we still need to finetune the model on the original dataset D. However, this finetuning step often suffers from slow convergence speed, which significantly slows down the learning process.To mitigate this problem, we add an additional latent distribution training step where we only finetune parameters correspond to p(Z). In this way, we only need to propagate training samples through the sub-PCs correspond to the latent-conditioned distributions once. After this step converges, we move on to finetune the whole model, which then takes much fewer epochs to converge."
method_iclr_2023_2_3 = "\n This section discusses how to induce assignments to LVs using expressive generative models. While the answer is specific to individual data types, we proposes preliminary answers of the question in the context of image data. We highlight that there are many possible LV selection strategies and target generative model; the following method is only an example that shows the effectiveness of LVD.Motivated by recent advances of image-based deep generative models (Dosovitskiy et al., 2020; Liu et al., 2021) , we model images by two levels of hierarchy -the low-level models independently encode distribution of every image patch, and the top-level model represents the correlation between different patches. Formally, we define X i as the variables in the ith M \u00d7 M patch of an H \u00d7 W image (w.l.o.g. assume H and W are both divisible by M ). Therefore, the imageX is divided into k = H \u2022 W/M 2 subsets {X i } k i=1 .Every Z i is defined as the LV corresponds to patch X i . Recall that our goal is to obtain the assignment of {Z i } k i=1 , each as a concise representation of {X i } k i=1 , respectively. Despite various possible model choices, we choose to use Masked Autoencoders (MAEs) (He et al., 2022) as they produce good features for image patches. Specifically, as shown in Figure 6 (a), MAE consists of an encoder and a decoder. During training, a randomly selected subset of patches are fed to the encoder to generate a latent representation for every patch. The features are then fed to the decoder to reconstruct the full image. The simplest way to compute latent features for every patch is to feed them into the encoder independently, and extract the corresponding features. However, we find that it is beneficial to also input other patches as context. Specifically, we first compute the latent features without context. We then compute the correlation between features of all pair of patches and construct the Maximum Spanning Tree (MST) using the pairwise correlations. Finally, to compute the feature of each patch X i , we additionally input patches correspond to its ancestors in the MST. Further indices as the LV assignments. Figure 6 (b) shows some example image patches x 1 belonging to four latent clusters (i.e., Z 1 = 1, . . . , 4). Clearly, the LVs capture semantics of different image patches.To illustrate the effectiveness of LVD, we make minimum structural changes compared to Hidden Chow-Liu Trees (HCLTs) (Liu & Van den Broeck, 2021) , a competitive PC structure. Specifically, we use the HCLT structure for all sub-PCs {p(x i |Z i = j)} i,j and p(z). This allows us to materialize patch-based LVs while keeping the model architecture similar to HCLTs."
method_iclr_2023_2 = (
    method_iclr_2023_2_0
    + method_iclr_2023_2_1
    + method_iclr_2023_2_2
    + method_iclr_2023_2_3
)
experiments_iclr_2023_2 = "Experiments and Results: In this section, we evaluate the proposed latent variable distillation (LVD) technique on three natural image benchmarks, i.e., CIFAR (Krizhevsky et al., 2009) and two versions of down-sampled ImageNet (ImageNet32 and ImageNet64) (Deng et al., 2009) . On all benchmarks, we demonstrate the effectiveness of LVD from two perspectives. First, compared to PCs trained by existing EM-based optimizers, the proposed technique offers a significant performance gain especially on large PCs. Second, PCs trained by LVD achieve competitive performance against some of the less tractable deep generative models, including variational autoencoders and flow-based models.Baselines We compare the proposed method against three TPM baselines: Hidden Chow-Liu Tree (HCLT) (Liu & Van den Broeck, 2021) , Einsum Network (EiNet) (Peharz et al., 2020a) , and Random Sum-Product Network (RAT-SPN) (Peharz et al., 2020b) . Though not exhausive, this baseline suite embodies many of the recent advancement in tractable probabilistic modeling, and can be deemed as the existing SoTA. To evaluate the performance gap with less tractable deep generative models, we additionally compare LVD with the following flow-based and VAE models: Glow (Kingma & Dhariwal, 2018), RealNVP (Dinh et al., 2016) , and BIVA (Maal\u00f8e et al., 2019) .To facilitate a fair comparison with the chosen TPM baselines, we implement both HCLT and RAT-SPN using the Julia package Juice.jl (Dang et al., 2021) and tune hyperparameters such as batch size, learning rate and its schedule. We use the original PyTorch implementation of EiNet and similarly tune their hyperparameters. For all TPMs, we train various models with number of parameters ranging from \u223c1M to \u223c100M, and report the number of the model with the best performance. For deep generative model baselines, we adopt the numbers reported in the respective original papers. Please refer to Appx. C for more details of the experiment setup.We first compare the performance of the four TPM approaches. As shown in Figure 1 , for all three benchmarks, PCs trained by LVD are consistently better than the competitors by a large margin. In particular, on ImageNet32, a \u223c25M PC trained by LVD is better than a HCLT with \u223c400M parameters. Next, looking at individual curves, we observe that with LVD, the test set bpd keeps decreasing as the model size increases. This indicates that LVD is able to take advantage of the extra capacity offered by large PCs. In contrast, PCs trained by EM immediately suffer from a performance bottleneck as the model size increases. Additionally, the efficient LVD learning pipeline described in Section 4 allows us to train PCs with 500M parameters in 10 hours with a single NVIDIA A5000 GPU, while existing optimizers need over 1 day to train baseline PCs with similar sizes. Please refer to Appx. D for detailed analysis of the computation efficiency of LVD.We move on to compare the performance of LVD with the three adopted DGM baselines. As shown in Table 1 , although the performance gap is relatively large on CIFAR, the performance of LVD is highly competitive on ImageNet32 and ImageNet64, with bpd gap ranging from \u223c 0.1 to \u223c 0.3. We hypothesize that the relatively large performance gap on CIFAR is caused by insufficient training samples. Specifically, for the PC structures specified in Section 5, the sub-PCs correspond to the latent-conditioned distributions {p(x i |Z i = j)} i,j are constructed independently, and thus every training sample x i can only be used to train its corresponding latent-conditioned distribution, making the model extremely data-hungry. However, we note that this is not an inherent problem of LVD. For example, by performing parameter tying of sub-PCs correspond to different image patches, we can significantly improve sample complexity of the model. This is left to future work."
conclusion_iclr_2023_2 = "Conclusion: Scaling probabilistic circuits to large and high-dimensional real-world datasets has been a key challenge: as the number of parameters increases, their performance gain diminishes immediately.In this paper, we propose to tackle this problem by latent variable distillation: a general framework for training probabilistic circuit that provides extra supervision over their latent spaces by distilling information from existing deep generative models. The proposed framework significantly boosts the performance of large probabilistic circuits on challenging benchmarks for both image and language modeling. In particular, with latent variable distillation, a image-patch-structured probabilistic circuit achieves competitive performance against flow-based models and variational autoencoders. Despite its empirical success on scaling up probabilistic circuits, at high-level, latent variable distillation also implies a new way to organically combine probabilistic circuits and neural models, opening up new avenues for tractable generative modeling.Reproducibility statement To facilitate reproducibility, we provide detailed description of the models and training details in both the main text and the appendix. Specifically, the last paragraph in Section 5 elaborates the PC structure used for image modeling tasks; training details of both the proposed method and the baselines are provided in Section 6 and Appx. C. For baselines, we always use the official GitHub implementation if possible. For our method, we provide detailed explanation of all hyperparameters used in the experiment (Appx. C)."

paper_text_iclr_2023_2 = f"{title_iclr_2023_2}\n {abstract_iclr_2023_2}\n {introduction_iclr_2023_2}\n {related_work_iclr_2023_2}\n{method_iclr_2023_2}\n{experiments_iclr_2023_2} \n {conclusion_iclr_2023_2}"

review_1_iclr_2023_2 = "paper_summary: This paper presents an approach to scale tractable probabilistic models via latent variable extensions. In particular, the authors extend the naive factorizations of models such as an SPN to introduce a new latent variable that comes from the embedding of a more complex neural network.\n\nThis extended model retains the tractability properties while reducing the bits-per-dimension for both image and text datasets compared to other state-of-the-art tractable probabilistic models.\n\nThe authors present a distributed learning approach that uses the independencies induced by the conditioning to accelerate the training, scaling the capabilities of TPMs.\n \
                        main_review: The paper is easy to read, and introduces the idea clearly. It is a novel approach that makes TPMs even more scalable.\n\nThe algorithms presented are well defined and allows for ease of reproducibility. \
                        strength_weakness: The authors present a well-written and technically sound paper. \n\nThe empirical evaluation is outstanding for TPMs, both in terms of scale and model performance.\n\nIt would be nice to have some more comments on the impact of LVD with regards to the rest of the model. How would you do imputation? How is the performance of your model when you marginalize the LVs?\n\n \n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: I find the paper very interesting and it opens the door to integrate other more complex models while retaining some of the tractability properties. \n\nThis is a significant contribution, although I'm wondering about downside of this approach or some comments on the questions raised regarding the use of the model when we don't have access or can't compute the LVs.\n\nThe bpd results are very impressive. I'm also wondering about how this model differs from the one presented by Shao 2022, in the case you would set the latent variables and the conditional part. \n\n \
                        score: 0.77 \n \
                        confidence: 1.0 \n \
                        novelty: 0.75 \n \
                        correctness: 1.0 "


review_2_iclr_2023_2 = 'paper_summary: Probabilistic circuits (PCs) are a unified framework that encompasses a number of tractable probabilistic models, such as arithmetic circuits and sum-product networks. While recent work has been able to scale them by means of parallelization and vectorization, their performance does not scale with the number of parameters. This rather unintuitive results suggest that the problem comes from the optimization process, which cannot fully utilize the model parameters and gets stuck in local optima. This work proposes to leverage existing deep generative models to assist the training of PCs through a novel method called latent variable distillation. In short, the PC is extended to include a set of latent variables, which are instantiated via the latent representation from deep generative models. This alone permits to better exploit the model. Additionally, said latent extension permits to divide the PCs into conditionally independent modules which can be trained simultaneously, speeding up the training process. The authors demonstrate the advantages of the proposed method in the text and image domains by leveraging Bert and MAE models. \
                        main_review: Clarity and quality. I find the paper very well written and of high quality. There are a number of typos that should be corrected for the next revision. I also find the paper to reiterate the same statements too many times, but this is personal taste.\n\n**Novelty.** The method seems novel to my eyes, and the advantages are clear. However, I find _really_ surprising that there are no citations at all regarding Probabilistic Circuits. They were first introduced in a UAI tutorial, and afterwards [a technical report](https://web.cs.ucla.edu/~yjchoi/publications/ProbCirc20/) was written by the authors which can be cited.\n\n**Reproducibility.** The experimental set-up should be reproducible. \n \
                        strength_weakness: Strengths:\n- S1. This work clearly demonstrates a novel method to exploit all the capacity of PCs using auxiliary and non-tractable methods from Deep Learning. This is extremely relevant for the PC community to close the gap with existing DL approaches.\n- S2. The practical speed-up obtained by their method (section 4) is quite relevant as well.\n- S3. Experimental results are strong.\n\n**Weaknesses:**\n- W1. While the idea is novel, I felt a bit let down when I read that the approach need to be "engineered" depending on the dataset, architecture, and deep generative model. The two instantiations of the approach look really ad-hoc. I would\'ve hoped to see some sort of general guidelines, or desiredata for the method to be successful.\n- W2. On a similar note, I feel a deeper ablation study would help a lot. There are too many questions open:\n   - How important is the deep generative architecture to obtain good results?\n   - Do other PCs architectures benefit from the proposed approach? So far, it is only tried in a modified version of HCLT.\n   - K-means is also prone to fall into local optima. How much do the results vary with different K-means initializations?\n   - How does the algorithm perform if we condition (Fig. 5) on deeper layers?\n- W3. I might have missed it, but it is not clear to me how many times each experiment was repeated. In any case, standard deviations are not reported.\n- W4. No samples were reported, so one cannot assess the qualitative improvements. Are the new generated samples better as well?\n\n**Questions:**\n- Q1. Any intuition on why the order to generate the latent samples is reversed with respect to the HMM in the example of section 2?\n- Q2. Could you clarify why is the upper part of Fig. 5 supposed to be $p(z_1, z_2)$. While I agree on the bottom modules, I do not see how the top part represents the marginal of $z$.\n- Q3. I understand LVD is HCLT using the proposed training approach. Why is there so much difference in number of parameters between both approaches in Figure 7? Is it because of the "minimum modifications required" described in the text? Why not comparing with that modified architecture without LVD?\n-  Q4. What do you mean by "semantics" in the manuscript? I really struggle to understand what "semantic-aware" means.\n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: review_summary: The paper is novel and well-written. The empirical results clearly demonstrate the advantages of the proposed method with respect to existing approaches. However, I feel the paper falls short when it comes to understanding the influence of the different factors of the proposed method in the final performance. \
                        score: 0.77 \
                        confidence: 0.8 \
                        novelty: 1.0 \
                        correctness: 1.0 '

review_3_iclr_2023_2 = "paper_summary: This paper is about improving the expressivity of large scale probabilistic circuits (PCs). Finding a good starting point for EM based learning of these large latent variable models is problematic and the authors propose one such solution to this problem.  The main idea is to obtain semantic-aware assignments (called supervision) to the latent variables from less tractable deep generative models and then perform maximum likelihood learning over the data combined with these newly assigned latent variables. The variables that receive these assignments are said to be materialized and the assignments themselves are generated by a deep generative model by clustering over the latent embeddings of the observed (sub)space(s). When all the latent variables have been assigned values, the optimization (MLE) can be performed in closed form. The result of MLE serves as a starting point for optimizing the data likelihood in the following steps. The authors propose a couple of techniques to efficiently compute the MLE parameters which include exploiting conditional independency achieved by materialized latent variables and fine tuning the latent distributions only while keeping the parameters learned over the observed space fixed. On CIFAR and ImageNet datasets, the proposed method has shown superior performance compared to other SoTA TPMs learners and were comparable to less tractable but expressive flow-based models and VAEs.   \n \
                        main_review: The paper is well written with illustrative examples. I found the idea to be novel and the authors have detailed their experimental setups for reproducibility. \
                        strength_weakness: Strengths:\n1) a novel method to improve the performance of large scale probabilistic circuits.\n2) motivating empirical evaluations on three image datasets to show the performance improvements over very large circuits. \n3) clear writeup with good examples. \nWeaknesses:\nI didn't find major weaknesses in the technical aspects of the paper. Please see some questions and comments below. \n \
                        questions: / \n \
                        limitations: / \n \
                        review_summary: The authors have addressed an important practical issue with large scale probabilistic circuits. The expressivity of these models tend to plateau once a certain capacity is reached typically in the order of millions of parameters. With such large scale circuits of deeply nested latent variables, the optimization landscape becomes very complex and finding a local minima becomes hard. The main idea in the paper is to make latent variables observed by assigning them values and performing an optimization step that works on less number of latent variables. This will give the EM step a good starting point. I found the idea to obtain latent variable assignments using a deep generative model to be interesting. The method seems to be effective according to the empirical results presented in the paper. \nQuestions:\na) In the introduction it is stated that the expressive power of PCs should monotonically increase with respect to the number of parameters. I am curious if these models don't suffer from overfitting issues. Maybe the authors could comment on this aspect.  \nb) Did all the models have the same structure?\nc) Could the method be useful for smaller scale PCs? It seems that the clustering in the latent embedding is the key reason behind the performance boost of LVDs.\nd) The significant speed up in training (for all the datasets) should probably be presented in the paper since it is mentioned in the introduction. \ne) Have you done any analysis on the number of LVs that were materialized and the performance of the PCs?\n \
                        score: 0.77 \n \
                        confidence: 0.8 \n \
                        novelty: 1.0 \n \
                        correctness: 0.75 "


review_text_iclr_2023_2 = f"Review 1: {review_1_iclr_2023_2}\n Review 2: {review_2_iclr_2023_2}\n Review 3: {review_3_iclr_2023_2} "


# Data for Paper 1 of NeurIPS 2023
title_neurips_2023_1 = "Title:"
abstract_neurips_2023_1 = "Abstract:"
introduction_neurips_2023_1 = 'Introduction: An important challenge in machine learning is the integration and translation of data across multiple domains (Zhu et al., 2017; Zhuang et al., 2021) . Researchers often have access to large amounts of unpaired data from several domains, e.g., images and text. It is then desirable to learn a probabilistic coupling between the observed marginal distributions that captures the relationship between the domains. One approach to tackle this problem is to assume that there is a latent representation that is invariant across the different domains (Bengio et al., 2013; Ericsson et al., 2022) . Finding a probabilistic coupling then boils down to learning such a latent representation, that is, learning high-level, latent variables that explain the variation of the data within each domain as well as similarities across domains.In traditional representation learning, the latent variables are assumed to be statistically independent, see for example the literature on independent component analysis (Hyv\u00e4rinen and Oja, 2000; Comon and Jutten, 2010; Khemakhem et al., 2020) . However, the assumption of independence can be too stringent and a poor match to reality. For example, the presence of clouds and the presence of wet roads in an image may be dependent, since clouds may cause rain which may in turn cause wet roads. Thus, it is natural to seek a causal representation, that is, a set of high-level causal variables and relations among them (Sch\u00f6lkopf et al., 2021; Yang et al., 2021b) . Figure 1 illustrates the setup of multi-domain causal representation learning, where multiple domains provide different views on a shared causal representation.Our motivation to study multi-domain causal representations comes, in particular, from single-cell data in biology. Given a population of cells, different technologies such as imaging and sequencing provide different views on the population. Crucially, since these technologies destroy the cells, the observations are uncoupled, i.e., a specific cell may either be used for imaging or sequencing but not both. The aim is to integrate the different views on the population to study the underlying causal mechanisms determining the observed features in various domains (Butler et al., 2018; Stuart et al., 2019; Liu et al., 2019; Yang et al., 2021a; Lopez et al., 2022; Gossi et al., 2023; Cao et al., 2022) . Unpaired multi-domain data also appears in many applications other than single-cell biology. For example, images of similar objects are captured in different environments (Beery et al., 2018) , large biomedical and neuroimaging data sets are collected in different domains (Miller et al., 2016; Essen et al., 2013; Shafto et al., 2014; Wang et al., 2003) , or stocks are traded in different markets.In this paper, we study identifiability of the shared causal representation, that is, its uniqueness in the infinite data limit. Taking on the same perspective as, for example, in Sch\u00f6lkopf et al. (2021) and Squires et al. (2023) , we assume that observed data is generated in two steps. First, the latent variables Z = (Z i ) i\u2208H are sampled from a distribution P Z , where P Z is determined by an unknown structural causal model among the latent variables. Then, in each domain e \u2208 {1, . . . , m}, the observed vector X e \u2208 R de is the image of a subset of the latent variables under a domain-specific, injective mixing function g e . That is,X e = g e (Z Se ),where S e \u2286 H is a subset of indices. A priori, it is unknown whether a latent variable Z i with i \u2208 S e is shared across domains or domain-specific. Even the number of latent variables which are shared across domains is unknown. Moreover, we only observe the marginal distribution of each random vector X e , but none of the joint distributions over pairs X e , X f for e \u0338 = f . Said differently, observations across domains are unpaired. Assuming that the structural causal model among the latent variables as well as the mixing functions are linear, our main contributions are:1. We lay out conditions under which we can identify the joint distribution of X 1 , . . . , X m . 2. We give additional conditions under which we are able to identify the causal structure among the shared latent variables.In particular, identifiability of the joint distribution across domains enables data translation. That is, given observation x in domain e, translation to domain f can be achieved by computing E[X f |X e = x]. Furthermore, identifying the causal structure among the shared latent variables lets us study the effect of interventions on the different domains.The main challenge in proving rigorous identifiability results for multi-domain data is that we cannot apply existing results for single-domain data in each domain separately. Even if the causal structure of the latent variables in a single domain is identifiable, it remains unclear how to combine multiple causal structures, i.e., in which way latent variables are shared. We circumvent this problem via a two-step approach: First, we extend the identifiability of linear independent component analysis (Comon, 1994; Hyv\u00e4rinen and Oja, 2000; Eriksson and Koivunen, 2004; Mesters and Zwiernik, 2022) to the multi-domain setup, which allows us to identify the joint distribution and distinguish between shared and domain-specific latent variables. Moreover, we identify an "overall mixing matrix" and, in a second step, exploit sparsity constraints in this matrix to identify the causal structure among the shared latent variables. This leverages recent results on causal discovery under measurement error in single domains that also exploit sparsity (Xie et al., 2020; Chen et al., 2022; Xie et al., 2022; Huang et al., 2022) . Although we emphasize that our focus in this paper is on identifiability, our proofs also suggest methods to learn the joint distribution as well as the shared causal graph from finite samples. We provide algorithms for the noisy setting and, moreover, we analyze how the number of domains reduce uncertainty with respect to the learned representation.The paper is organized as follows. In the next paragraphs we discuss further related work. Section 2 provides a precise definition of the considered setup. In Section 3 we consider identifiability of the joint distribution. Using these results, we study identifiability of the causal graph in Section 4. We conclude with a small simulation study as a proof of concept for the finite sample setting in Section 5. Due to space constraints, the detailed discussion of the finite sample setting is deferred to the Appendix. Moreover, the Appendix contains all proofs, discussions on the necessity of our assumptions, and additional examples and simulation results.Multi-domain Integration. Motivated by technological developments for measuring different modalities at single-cell resolution, several methods have been proposed recently for domain translation between unpaired data. The proposed methods rely on a variety of techniques, including manifold alignment (Welch et al., 2017; Amodio and Krishnaswamy, 2018; Liu et al., 2019) , matrix factorization (Duren et al., 2018) , correlation analysis (Barkas et al., 2019; Stuart et al., 2019) , coupled autoencoders (Yang and Uhler, 2019) , optimal transport (Cao et al., 2022) , regression analysis (Yuan and Duren, 2022) , and semisupervised learning (Lin et al., 2022) . Implicitly, these methods presume the existence of a shared latent space where the different modalities either completely align or at least overlap. However, to the best of our knowledge, none of these methods have rigorous identifiability guarantees, i.e., the methods are not guaranteed to recover a correct domain translation mapping even for infinite data. Our work advances the theoretical understanding of multi-domain integration by providing identifiability guarantees on recovering the shared latent space.Group Independent Component Analysis. The primary tool that we use for identifiability is linear independent component analysis (ICA) (Comon, 1994; Eriksson and Koivunen, 2004) . Many works extend ICA to the multi-domain setting. These methods primarily come from computational neuroscience, where different domains correspond to different subjects or studies. However, to the best of our knowledge, all prior works require pairing between samples. These works can be categorized based on whether the samples are assumed to be voxels (Calhoun et al., 2001; Esposito et al., 2005) , time points (Svens\u00e9n et al., 2002; Varoquaux et al., 2009; Hyv\u00e4rinen and Ramkumar, 2013) , or either (Beckmann and Smith, 2005; Sui et al., 2009) . For reviews, see Calhoun et al. (2009) and Chabriel et al. (2014) . Related are methods for independent vector analysis (Kim et al., 2006; Anderson et al., 2014; Bhinge et al., 2019) and multiset canonical correlation analysis (Nielsen, 2002; Li et al., 2011; Klami et al., 2014) , which allow the latent variables to take on different values in each domain but still require sample pairing. Most of the mentioned methods lack identifiability guarantees, only newer work (Richard et al., 2021) provides sufficient conditions for identifiability. Furthermore, all mentioned methods assume that every latent variable is shared across all domains, while our setup allows for shared and domain-specific latent variables. Some methods, e.g., Lukic et al. (2002) , Maneshi et al. (2016) , and Pandeva and Forr\u00e9 (2023) , permit both shared and domain-specific components, but only consider the paired setting. In this paper, we extend these results to the unpaired setting.Latent Structure Discovery. Learning causal structure between latent variables has a long history, e.g., in measurement models (Silva et al., 2006) . One recent line of work studies the problem under the assumption of access to interventional data (e.g., Liu et al., 2022; Squires et al., 2023) . In particular, Squires et al. (2023) show that the latent graph is identifiable if the interventions are sufficiently diverse. Another line of work, closer to ours and not based on interventional data, shows that the graph is identified under certain sparsity assumptions on the mixing functions (Xie et al., 2020; Chen et al., 2022; Xie et al., 2022; Huang et al., 2022) . However, these methods are not suitable in our setup since they require paired data in a single domain. One cannot apply them in each domain separately since it would be unclear how to combine the multiple latent causal graphs, that is, which of the latent variables are shared. In this work, we lay out sparsity assumptions on the mixing functions that are tailored to the unpaired multi-domain setup. The works of Adams et al. (2021) and Zeng et al. (2021) may be considered closest to ours as they also treat a setting with multiple domains and unpaired data. However, our setup and results are more general. Adams et al. (2021) assume that the number of observed variables are the same in each domain, whereas we consider domains of different dimensions corresponding to the fact that observations may be of very different nature. Further, we allow for shared and domain-specific latent variables, where the number of shared latent variables is unknown, while in Adams et al. (2021) it is assumed that all latent variables are shared. Compared to Zeng et al. (2021) , we consider a general but fixed number of observed variables, while Zeng et al. (2021) only show identifiability of the full model in a setup where the number of observed variables in each domain increases to infinity. On a more technical level, the conditions in Zeng et al. (2021) require two pure children to identify the shared latent graph, while we prove identifiability under the weaker assumption of two partial pure children; see Section 4 for precise definitions. A matrix Q = Q \u03c3 \u2208 R p\u00d7p is asigned permutation matrix if it can be written as the product of a diagonal matrix D with entriesD ii \u2208 {\u00b11} and a permutation matrix Q \u03c3 with entries ( Q \u03c3 ) ij = 1 j=\u03c3(i), where \u03c3 is a permutation on p elements. Let P be a p-dimensional joint probability measure of a collection of random variables Y 1 , . . . , Y p . Then we denote by P i the marginal probability measure such that Y i \u223c P i . We say that P has independent marginals if the random variables Y i are mutually independent. Moreover, we denote by M #P the d-dimensional push-forward measure under the linear map defined by the matrix M \u2208 R d\u00d7p . If Q is a signed permutation matrix and the probability measure P has independent marginals, then Q#P also has independent marginals. For univariate probability measures we use the shorthand (-1)#P = -P .'
related_work_neurips_2023_1 = "Related Work:"

method_neurips_2023_1_1 = 'Method: Let H = [h] for h \u2265 1, and let Z = (Z 1 , . . . , Z h ) be latent random variables that follow a linear structural equation model. That is, the variables are related by a linear equation Z = AZ + \u03b5, (1) with h \u00d7 h parameter matrix A = (a ij ) and zero-mean, independent random variables \u03b5 = (\u03b5 1 , . . . , \u03b5 h ) that represent stochastic errors. Assume that we have observed random vectors X e \u2208 R de in multiple domains of interest e \u2208 [m], where the dimension d e may vary across domains. Each random vector is the image under a linear function of a subset of the latent variables. In particular, we assume that there is a subset L \u2286 H representing the shared latent space such that each X e is generated via the mechanismEQUATIONwhere I e \u2286 H \\ L. We say that the latent variable Z Ie are domain-specific for domain e \u2208 [m] while the latent variables Z L are shared across all domains. As already noted, we are motivated by settings where the shared latent variables Z L capture the key causal relations and the different domains are able to give us combined information about these relations. Likewise, we may think about the domainspecific latent variables Z Ie as "noise" in each domain, independent of the shared latent variables. Specific models are now derived from (1)-( 2) by assuming specific (but unknown) sparsity patterns in A and G e . Each model is given by a "large" directed acyclic graph (DAG) that encodes the multi-domain setup. To formalize this, we introduce pairwise disjoint index sets V 1 , . . . , V m , where V e indexes the coordinates of X e , i.e., X e = (X v : v \u2208 V e ) and|V e | = d e . Then V = V 1 \u222a \u2022 \u2022 \u2022 \u222a V mindexes all observed random variables. We define an m-domain graph such that the latent nodes are the only parents of observed nodes and there are no edges between shared and domain-specific latent nodes. Definition 2.1. Let G be a DAG whose node set is the disjoint unionH \u222a V = H \u222a V 1 \u222a \u2022 \u2022 \u2022 \u222a V m .Let D be the edge set of G. Then G is an m-domain graph with shared latent nodes L = [\u2113] \u2286 H if the following is satisfied:1. All parent sets contain only latent variables, i.e., pa(v) = {w :w \u2192 v \u2208 D} \u2286 H for all v \u2208 H \u222a V . 3 4 1 2 5 V 1 V 2 Figure 2:Compact version of a 2-domain graph G 2 with five latent nodes and two domains V 1 and V 2 . All observed nodes in each domain are represented by a single grey node. We draw a dashed blue edge from latent node h \u2208 H to domain V e \u2286 V if h \u2208 S e = pa(V e ). The random vectors associated to the two domains are uncoupled, that is, we do not observe their joint distribution.2. The set L consists of the common parents of variables in all different domains, i.e., u \u2208 L if and only if u \u2208 pa(v) \u2229 pa(w) for v \u2208 V e , w \u2208 V f with e \u0338 = f .Let I e = S e \\ L be the domain-specific latent nodes, where S e := pa(V e ) = \u222a v\u2208Ve pa(v) \u2286 H. Then there are no edges in D that connect a node in L and a node \u222a m e=1 I e or that connect a node in I e and a node in I f for any e \u0338 = f . To emphasize that a given DAG is an m-domain graph we write G m instead of G. We also say that S e is the set of latent parents in domain e and denote its cardinality by s e = |S e |. Note that the third condition in Definition 2.1 does not exclude causal relations between the domain-specific latent variables, that is, there may be edges v \u2192 w for v, w \u2208 I e . Since the sets I e satisfy I e \u2229 I f = \u2205 for e \u0338 = f , we specify w.l.o.g. the indexing convention I e = {\u2113 + 1 + \u03d5 Gm : R D V H \u00d7 R D HH -\u2192 R |V |\u00d7|H| (G, A) -\u2192 G \u2022 (I -A) -1 .Then the multi-domain causal representation (MDCR) model M(G m ) is given by the set of probability measures P X = B#P , where B \u2208 Im(\u03d5 Gm ) and P is an h-dimensional probability measure with independent, mean-zero marginals P i , i \u2208 H. We say that the pair (B, P ) is a representation of P X \u2208 M(G m ).Definition 2.3 corresponds to the model defined in Equations ( 1) and (2). If P X \u2208 M(G m ) with representation (B, P ), then P X is the joint distribution of the observed domains X = (X 1 , . . . , X m ). The distribution of the random variables \u03b5 i in Equation ( 1) is given by the marginals P i . Moreover, for any matrix G \u2208 R D V H , we denote the submatrix G e = G Ve,Se \u2208 R de\u00d7se which coincides with the matrix G e from Equation (2). For the graph in Figure 2 , we compute a concrete example of the matrix B in Example B.1 in the Appendix. Importantly, in the rest of the paper we assume to only observe the marginal distribution P X e in each domain but not the joint distribution P X .Ultimately, we are interested in recovering the graph G L = (L, D LL ) among the shared latent nodes. We proceed by a two-step approach: In Section 3 we recover the representation (B, P ) of the joint distribution P X . To be precise, we recover a matrix B that is equal to B up to certain permutations of the columns. Then we use the matrix B to recover the shared latent graph G L in Section 4 and show that recovery is possible up to trivial relabeling of latent nodes that appear in the same position of the causal order.'
method_neurips_2023_1_2 = 'To identify the joint distribution P X , we apply identifiability results from linear ICA in each domain separately and match the recovered probability measures P i for identifying which of them are shared, that is, whether or not i \u2208 L. Let G m be an m-domain graph with shared latent nodes L, and let P X \u2208 M(G m ) with representation (B, P ). Recall that B = G(I -A) -1 with G \u2208 R D V H and A \u2208 R D HH . We make the following technical assumptions.(C1) (Different error distributions.) The marginal distributions P i , i \u2208 H are non-degenerate, non-symmetric and have unit variance. Moreover, the measures are pairwise different to each other and to the flipped versions, that is, P i \u0338 = P j and P i \u0338 = -P j for all i, j \u2208 H with i \u0338 = j. Subsequently, we let d be a distance on the set of univariate Borel probability measures such that d(P i , P j ) \u0338 = 0 and d(P i , -P j ) \u0338 = 0 for i \u0338 = j.(C2) (Full rank of mixing.) For each e \u2208 [m], the matrix G e = G Ve,Se \u2208 R de\u00d7se is of full column rank.By not allowing symmetric distributions in Condition (C1), we assume in particular that the distributions of the errors are non-Gaussian. Non-Gaussianity together with the assumptions of pairwise different and non-symmetric error distributions allow us to extend the results on identifiability of linear ICA to the unpaired multi-domain setup and to identify the joint distribution.In particular, the assumption of pairwise different error distributions allows for "matching" the distributions across domains to identify the ones corresponding to the shared latent space. Non-symmetry accounts for the sign-indeterminacy of linear ICA when matching the distributions.We discuss the necessity of these assumptions in Remark 3.2 and, in more detail, in Appendix C. Note that Condition (C1) is always satisfied in a generic sense, that is, randomly chosen probability distributions on the real line are pairwise different and non-symmetric with probability one. Finally, Condition (C2) requires in particular that for each shared latent node k \u2208 L there is at least one nodev \u2208 V e in every domain e \u2208 [m] such that k \u2208 pa(v).Under Conditions (C1) and ( C2) we are able to derive a sufficient condition for identifiability of the joint distribution. Let SP (p) be the set of p \u00d7 p signed permutation matrices. We define the set of signed permutation block matrices:\u03a0 = {diag(\u03a8 L , \u03a8 I1 , . . . , \u03a8 Im ) : \u03a8 L \u2208 SP (\u2113) and \u03a8 Ie \u2208 SP (|I e |)} .Our main result is the following. Theorem 3.1 says that the matrix B is identifiable up to signed block permutations of the columns.Under the assumptions of Theorem 3.1 it holds that B# P is equal to P X . That is, the joint distribution of the domains is identifiable. Remark 3.2. While Theorem 3.1 is a sufficient condition for identifiability of the joint distribution, we emphasize that pairwise different error distributions are in most cases also necessary; we state the exact necessary condition in Proposition C.1 in the Appendix. Said differently, if one is willing to assume that conceptually different latent variables also follow a different distribution, then identification of these variables is possible, and otherwise (in most cases) not. Apart from pairwise different error distributions, non-symmetry is then required to fully identify the joint distribution whose dependency structure is determined by the shared latent variables. If the additional assumption on non-symmetry is not satisfied, then it is still possible to identify the shared, conceptually different latent variables, which becomes clear by inspecting the proofs of Theorem 3.1 and Proposition C.1. The non-identifiability of the joint distribution would only result in sign indeterminacy, that is, entries of the matrix B could have a flipped sign. Remark 3.3. By checking the proof of Theorem 3.1, the careful reader may notice that the statement of the theorem still holds true when we relax the third condition in the definition of an m-domain Linear ICA: Find the smallest value s e such that P X e = B e #P e for a matrix B e \u2208 R de\u00d7 se and an s e -dimensional probability measure P e with independent, mean-zero and unit-variance marginals P e i . 5: end for 6: Matching: Let \u2113 be the maximal number such that there are signed permutation matrices{Q e } e\u2208[m] satisfying d([(Q e ) \u22a4 #P e ] i , [(Q f ) \u22a4 #P f ] i ) = 0for all i = 1, . . . , \u2113 and for all f \u0338 = e. Let L = {1, . . . , \u2113}. 7: Construct the matrix B and the tuple of probability measures P given byB = \uf8eb \uf8ec \uf8ec \uf8ed [ B 1 Q 1 ] L [ B 1 Q 1 ] [ s1]\\ L . . . . . . [ B m Q m ] L [ B m Q m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f8 and P = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed [(Q 1 ) \u22a4 #P 1 ] L [(Q 1 ) \u22a4 #P 1 ] [ s1]\\ L . . . [(Q m ) \u22a4 #P m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 . 8: return ( \u2113, B, P ).graph. That is, one may allow directed paths from shared to domain-specific latent nodes but not vice versa. For example, an additional edge 1 \u2192 4 between the latent nodes 1 and 4 would be allowed in the graph in Figure 2 . In this case, the dependency structure of the domains is still determined by the shared latent space. However, the structural assumption that there are no edges between shared and domain-specific latent nodes is made for identifiability of the shared latent graph in Section 4.Remark 3.4. The computational complexity of Algorithm 1 depends on the complexity of the chosen linear ICA algorithm, to which we make m calls. Otherwise, the dominant part is the matching in Line 6 with worst case complexityO(m \u2022 max e\u2208[m] d 2 e ), where we recall that m is the number of domains and d e is the dimension of domain e.In Appendix D we state a complete version of Algorithm 1 for the finite sample setting. In particular, we provide a method for the matching in Line 6 based on the two-sample Kolmogorov-Smirnov test. For finite samples, there might occur false discoveries, that is, distributions are matched that are actually not the same. With our method, we show that the probability of falsely discovering shared nodes shrinks exponentially with the number of domains.'
method_neurips_2023_1_3 = "We return to our goal of identifying the causal graph G L = (L, D LL ) among the shared latent nodes. By Theorem 3.1, we can identify the representation (B, P ) of P X \u2208 M(G m ) from the marginal distributions. In particular, we recover the matrix B = B\u03a8 for a signed permutation block matrix \u03a8 \u2208 \u03a0. Moreover, we know which columns correspond to the shared latent nodes. That is, we know that the submatrix B L obtained by only considering the columns indexed byL = L = [\u2113] is equal to B L \u03a8 L , where \u03a8 L \u2208 SP (\u2113). Problem 4.1. Let B \u2208 Im(\u03d5 Gm ) for an m-domain graph G m with shared latent nodes L. Given B L = B L \u03a8 L with \u03a8 L a signed permutation matrix, when is it possible to identify the graph G L ?Recently, Xie et al. (2022) and Dai et al. (2022) show that, in the one-domain setting with independent additive noise, the latent graph can be identified if each latent variable has at least two pure children. We obtain a comparable result tailored to the multi-domain setup. Definition 4.2. Let G m = (H \u222a V, D) be an m-domain graph with shared latent nodes L \u2286 H. For k \u2208 L, we say that an observed nodev \u2208 V is a partial pure child of k if pa(v) \u2229 L = {k}. Algorithm 2 IdentifySharedGraph 1: Input: Matrix B * \u2208 R |V |\u00d7\u2113 . 2: Output: Parameter matrix A \u2208 R \u2113\u00d7\u2113 . 3: Remove rows B * i,L from the matrix B * that are completely zero. 4: Find tuples (i k , j k ) k\u2208L with i k \u0338 = j k such that (i) rank(B * {i k ,j k },L ) = 1 for all k \u2208 L and (ii) rank(B * {i k ,iq},L ) = 2 for all k, q \u2208 L with k \u0338 = q. 5: Let I = {i 1 , . . . , i \u2113 } and consider B * I,L \u2208 R \u2113\u00d7\u2113 . 6: Find two permutation matrices R 1 and R 2 such that W = R 1 B * I,L R 2 is lower triangular. 7:Multiply each column of W by the sign of its corresponding diagonal element. This yields a new matrix W with all diagonal elements positive. 8: Divide each row of W by its corresponding diagonal element. This yields a new matrix W \u2032 with all diagonal elements equal to one. 9: Compute A = I -( W \u2032 ) -1 . 10: return A.For a partial pure child v \u2208 V , there may still be domain-specific latent nodes that are parents of v. Definition 4.2 only requires that there is exactly one parent that is in the set L. This explains the name partial pure child; see Example B.2 in the Appendix for further elaboration. W.l.o.g. we assume in this section that the shared latent nodes are topologically ordered such that i \u2192 j \u2208 D LL implies i < j for all i, j \u2208 L. We further assume: (C3) (Two partial pure children across domains.) For each shared latent node k \u2208 L, there exist two partial pure children.(C4) (Rank faithfulness.) For any two subsets Y \u2286 V and W \u2286 L, we assume that rank(B Y,W ) = maxB \u2032 \u2208Im(\u03d5 Gm ) rank(B \u2032 Y,W ).The two partial pure children required in Condition (C3) may either be in distinct domains or in a single domain. This is a sparsity condition on the large mixing matrix G. In Appendix C we discuss that the identification of the joint latent graph is impossible without any sparsity assumptions. We conjecture that two partial pure children are not necessary, but we leave it open for future work to find a non-trivial necessary condition. Roughly speaking, we assume in Condition (C4) that no configuration of edge parameters coincidentally yields low rank. The set of matrices B \u2208 Im(\u03d5 Gm ) that violates (C4) is a subset of measure zero of Im(\u03d5 Gm ) with respect to the Lebesgue measure. Note that our conditions do not impose constraints on the graph G L . Our main tool to tackle Problem 4.1 will be the following lemma.Lemma 4.3. Let B \u2208 Im(\u03d5 Gm ) for an m-domain graph G m . Suppose that Condition (C4) is satisfied and that there are no zero-rows in B L . Let v, w \u2208 V . Then rank(B {v,w},L ) = 1 if and only if there is a node k \u2208 L such that both v and w are partial pure children of k.The condition on no zero-rows in Lemma 4.3 is needed since we always have rank(B {v,w},L ) \u2264 1 if one of the two rows is zero. However, this is no additional structural assumption since we allow zero-rows when identifying the latent graph; c.f. Algorithm 2. The lemma allows us to find partial pure children by testing ranks on the matrix B L . If (i 1 , j 1 ) and (i 2 , j 2 ) are partial pure children of two nodes in L, we make sure that these two nodes are different by checking that rank(B {i1,i2},L ) = 2.For a DAG G = (V, D), we define S(G) to be the set of permutations on |V | elements that are consistent with the DAG, i.e., \u03c3 \u2208 S(G) if and only if \u03c3(i) < \u03c3(j) for all edges i \u2192 j \u2208 D. The following result is the main result of this section.Theorem 4.4. Let B = B\u03a8 with B \u2208 Im(\u03d5 Gm ) and \u03a8 \u2208 \u03a0, and define B * = B L to be the input of Algorithm 2. Assume that Conditions (C3) and (C4) are satisfied, and let A be the output of Algorithm 2. ThenA = Q \u22a4 \u03c3 A L,L Q \u03c3 for a signed permutation matrix Q \u03c3 with \u03c3 \u2208 S(G L ). Moreover, if G vk > 0 for G \u2208 R D V H whenever v is a pure child of k, then Q \u03c3 is a permutation matrix.Theorem 4.4 says that the graph G L can be recovered up to a permutation of the nodes that preserves the property that i \u2192 j implies i < j; see Remark 4.5. Since the columns of the matrix B are not only permuted but also of different signs, we solve the sign indeterminacy column-wise in Line 7 before removing the scaling indeterminacy row-wise in Line 8. In case the coefficients of partial pure children are positive, this ensures that Q \u03c3 is a permutation matrix and we have no sign indeterminacy. In Appendix D we adapt Algorithm 2 for the empirical data setting, where we only have B L \u2248 B L \u03c8 L . Remark 4.5. Let A be the output of Alg. 2. Then we construct the graph G L = (L, D LL ) as the graph with edges j \u2192 i \u2208 D LL if and only ifA ij \u0338 = 0. Condition (C4) ensures that G L is equivalent to G L in the sense that there is a permutation \u03c3 \u2208 S(G L ) such that D LL = {\u03c3(i) \u2192 \u03c3(j) : i \u2192 j \u2208 D LL }.Example 4.6. As highlighted in the introduction, the unpaired multi-domain setup is motivated by applications from single-cell biology. For example, consider the domains of (i) gene expression and (ii) high-level phenotypic features extracted from imaging assays (e.g. McQuin et al., 2018) . We argue that the requirement of two partial pure children is justifiable on such data as follows. The condition requires, for example, that for each shared latent variable, (i) the expression of some gene depends only upon that shared latent variable plus domain-specific latent variables, and (ii) one of the high-level phenotypic features depends only on the same latent feature plus domain-specific latent variables. Many genes have highly specialized functions, so (i) is realistic, and similarly many phenotypic features are primarily controlled by specific pathways, so (ii) is justified. Remark 4.7. In Algorithm 2, we determine the rank of a matrix by Singular Value Decomposition, which has worst case complexity O(mn min{n, m}) for an m \u00d7 n matrix. Since Line 4 is the dominant part, we conclude that the worst case complexity of Algorithm 2 is O(|V | 2 \u2022 \u2113)."
method_neurips_2023_1 = (
    method_neurips_2023_1_1
    + "\n"
    + method_neurips_2023_1_2
    + "\n"
    + method_neurips_2023_1_3
)

experiments_neurips_2023_1_1 = 'Experiments and Results: In this section we report on a small simulation study to illustrate the validity of our adapted algorithms for finite samples (detailed in Appendix D). We emphasize that this should only serve as a proof of concept as the focus of our work lies on identifiability. In future work one may develop more sophisticated methods; c.f. Appendix G. The adapted algorithms have a hyperparameter \u03b3, which is a threshold on singular values to determine the rank of a matrix. In our simulations we use \u03b3 = 0.2. Data Generation. In each experiment we generate 1000 random models with \u2113 = 3 shared latent nodes. We consider different numbers of domains m \u2208 {2, 3} and assume that there are |I e | = 2 domain-specific latent nodes for each domain. The dimensions are given by d e = d/m for all e \u2208 [m] and d = 30. We sample the m-domain graph G m on the shared latent nodes as follows. First, we sample the graph G L from an Erd\u0151s-R\u00e9nyi model with edge probability 0.75 and assume that there are no edges between other latent nodes, that is, between L and H \\ L and within H \\ L. Then we fix two partial pure children for each shared latent node k \u2208 L and collect them in the set W . The remaining edges from L to V \\W and from H to V are sampled from an Erd\u0151s-R\u00e9nyi model with edge probability 0.9. Finally, the (nonzero) entries of G and A are sampled from Unif(\u00b1[0.25, 1]). The distributions of the error variables are specified in Appendix E. For simplicity, we assume that the sample sizes coincide, that is, n e = n for all e \u2208 [m], and consider n \u2208 {1000, 2500, 5000, 10000, 25000}.Results. First, we plot the average number of shared nodes \u2113 in our experiments in Figure 3 (a) . Especially for low sample sizes, we see that fewer shared nodes are detected with more domains. However, by inspecting the error bars we also see that the probability of detecting too many nodes \u2113 > \u2113 decreases drastically when considering 3 instead of 2 domains. This suggests that the number of falsely detected shared nodes is very low, as expected by Theorem D.3. Our findings show that more domains lead to a more conservative discovery of shared nodes, but whenever a shared node is determined this is more certain. Moreover, we measure the error in estimating B L in Figure 3 (b), that is, the error in the "shared" columns. We takescore B ( B L ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 min \u03a8\u2208SP (\u2113) \u03b2 -1/2 \u2113, \u2113 \u2225 B L -[B L \u03a8] L \u2225 F if \u2113 \u2264 \u2113, min \u03a8\u2208SP ( \u2113) \u03b2 -1/2 \u2113, \u2113 \u2225[ B L \u03a8] L -B L \u2225 F if \u2113 > \u2113,where \u2225 \u2022 \u2225 F denotes the Frobenius norm and \u03b2 \u2113, \u2113 = min{\u2113, \u2113} \u2022 m e=1 d e denotes the number of entries of the matrix over which the norm is taken. In the cases \u2113 = \u2113, we also measure the Q\u03c3\u2208SP (\u2113) s.t. \u03c3\u2208S(G L ) 1 \u2113 \u2225Q \u22a4 \u03c3 AQ \u03c3 -A L,L \u2225 F .As expected, the median estimation errors for B L and A L,L decrease with increasing sample size. In Appendix F we provide additional simulations with larger \u2113. Moreover, we consider setups where we violate specific assumptions, such as pairwise different distributions (C1) and two partial pure children (C3). The results emphasize that the conditions are necessary for the algorithms provided. The computations were performed on a single thread of an Intel Xeon Gold 6242R processor (3.1 GHz), with a total computation time of 12 hours for all simulations presented in this paper (including Appendix).'
experiments_neurips_2023_1_2 = 'This work introduces the problem of causal representation learning from unpaired multi-domain observations, in which multiple domains provide complementary information about a set of shared latent nodes that are the causal quantities of primary interest. For this problem, we laid out a setting in which we can provably identify the causal relations among the shared latent nodes. To identify the desired causal structure, we proposed a two-step approach where we first make use of linear ICA in each domain separately and match the recovered error distributions to identify shared nodes and the joint distribution of the domains. In the second step, we identify the causal structure among the shared latent variables by testing rank deficiencies in the "overall mixing matrix" B. To the best of our knowledge, our guarantees are the first principled identifiability results for shared causal representations in a general, unpaired multi-domain setting.We proposed algorithms for recovering the joint distribution and the shared latent space making our proofs constructive. While our focus is on identifiability guarantees, we show in Appendix D how our proofs give rise to algorithms for the finite sample setting. Moreover, we propose a method to match approximate error distributions and show that the probability of falsely discovering shared nodes decreases exponentially in the number of domains. Our work opens up numerous directions for future work as we discuss in Appendix G.Proof In particular, the equality P X e = B e #P Se shows that the representation in Line 4 of Algorithm 1 exists. Now, we show that it is unique up to signed permutation by applying results on identifiability of linear ICA. Since G e has full column rank by Condition (C2) and [(I -A) -1 ] Se,Se is invertible, the matrix B e also has full column rank. Let P X e = B e #P e be any representation, where B e \u2208 R de\u00d7 se and P e is an s e -dimensional probability measure with independent, non-degenerate marginals P e i . Due to Condition (C1), all probability measures P i are non-Gaussian and non-degenerate and therefore we have by Eriksson and Koivunen (2004, Theorem 3 and 4) the identities B e = B e R e \u039b e and P e = \u039b e (R e ) \u22a4 #P Se ,where \u039b e is an s e \u00d7 s e diagonal matrix with nonzero entries and R e is an s e \u00d7 s e permutation matrix.In particular, we have s e = s e , which means that B e \u2208 R de\u00d7se and that P e is an s e -dimensional probability measure. Line 4 also requires that each marginal P e i has unit variance. This removes the scaling indeterminacy in (3) and we have B e = B e R e D e and P e = D e (R e ) \u22a4 #P Se ,where D e is a diagonal matrix with entries D e ii \u2208 {\u00b11}. In particular, this means that the distributions P e and P Se coincide up to permutation and sign of the marginals.The matching in Line 6 identifies which components of P e are shared. By Condition (C1), two components of different domains P e i and P f j are shared if and only if they coincide up to sign, that is, if and only if d(P e i , P f j ) = 0 or d(P e i , -P f j ) = 0. If their distribution coincide up to sign, than either d(P e i , P f j ) = 0 or d(P e i , -P f j ) = 0 but not both since Condition (C1) requires the distribution of the error variables to be non-symmetric. We conclude that in each domain e \u2208 [m] there exists an s e \u00d7 s e signed permutation matrixEQUATIONfor all i = 1, . . . , \u2113 and for all f \u0338 = e. In particular, \u2113 = \u2113 and L = L.It remains to show that B = B\u03a8 and P = \u03a8 \u22a4 #P for a signed permutation block matrix \u03a8 \u2208 \u03a0. By Equation (4), the distributions [(Q e ) \u22a4 #P e ] L and [(Q e ) \u22a4 #P e ] L coincide, which means thatEQUATIONwhere \u03a8 L is an \u2113 \u00d7 \u2113 signed permutation matrix and \u03a8 Ie is an |I e | \u00d7 |I e | signed permutation matrix. Importantly, the matrix \u03a8 \u22a4 L does not depend on the domain e \u2208 [m]. Hence, the matrix \u03a6 e := R e D e Q e is a signed permutation matrix with block structure as in Equation ( 5). Moreover, we haveB e Q e = B e R e D e Q e = B e \u03a6 e = B e L \u03a8 L B e [se]\\L \u03a8 Ie ,which means that the matrix B can be factorized asB = \uf8eb \uf8ec \uf8ec \uf8ed [ B 1 Q 1 ] L [ B 1 Q 1 ] [ s1]\\ L . . . . . . [ B m Q m ] L [ B m Q m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ed B 1 L \u03a8 L B 1 [s1]\\L \u03a8 I1 . . . . . . B m L \u03a8 L B m [sm]\\L \u03a8 Im \uf8f6 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ed B 1 L B 1 [s1]\\L . . . . . . B m L B m [sm]\\L \uf8f6 \uf8f7 \uf8f8 \u2022 \uf8eb \uf8ec \uf8ec \uf8ed \u03a8 L \u03a8 I1 . . . \u03a8 Im \uf8f6 \uf8f7 \uf8f7 \uf8f8 = B \u2022 \u03a8,where \u03a8 \u2208 \u03a0. Similarly, we have for all e \u2208 [m],(Q e ) \u22a4 #P e = (\u03a6 e ) \u22a4 #P Se = (\u03a8 L ) \u22a4 #P L (\u03a8 Ie ) \u22a4 #P Ie .We conclude thatP = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed [(Q 1 ) \u22a4 #P 1 ] L [(Q 1 ) \u22a4 #P 1 ] [ s1]\\ L . . . [(Q m ) \u22a4 #P m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed (\u03a8 L ) \u22a4 #P L (\u03a8 I1 ) \u22a4 #P I1 . . . (\u03a8 Im ) \u22a4 #P Im \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ed \u03a8 L \u03a8 I1 . . . \u03a8 Im \uf8f6 \uf8f7 \uf8f7 \uf8f8 \u22a4 # \uf8eb \uf8ec \uf8ec \uf8ed P L P I1 . . . P Im \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \u03a8 \u22a4 #P.Before proving Lemma 4.3 and Theorem 4.4 we fix some notation. Let G m = (V \u222a H, D) be an m-domain graph. We denote by anc(v) = {k \u2208 H : there is a directed path k \u2192 \u2022 \u2022 \u2022 \u2192 v in G m }the ancestors of a node v \u2208 V . For subsets W \u2286 V , we denote anc(W ) = v\u2208W anc(w). Moreover, for L \u2286 H and v \u2208 V , we write shortly pa L (v) = pa(v) \u2229 L.Proof of Lemma 4.3. Let B \u2208 Im(\u03d5 Gm ). Then we can writeB = G \u2022 (I -A) -1 with G = \uf8eb \uf8ec \uf8ed G V1,L G V1,I1 . . . . . . G Vm,L G Vm,Im \uf8f6 \uf8f7 \uf8f8.Moreover, observe that, by the definition of an m-domain-graph, the matrix B V,L factorizes asB V,L = G V,L [(I -A) -1 ] L,L .Now, suppose that i and j are partial pure children of a fixed node k \u2208 L. Then pa L (i) = {k} = pa L (j). In particular, the only entry that may be nonzero in the row G i,L is given by G ik and the only entry that may be nonzero in the row G j,L is given by G jk . Thus, we haveB i,L = q\u2208L G iq [(I -A) -1 ] q,L = G ik [(I -A) -1 ] k,L .Similarly, it follows that B j,L = G jk [(I -A) -1 ] k,L . This means that the row B j,L is a multiple of the row B i,L and we conclude that rank(B {i,j},L ) \u2264 1. Equality holds due to the faithfulness condition (C4) which implies that B ik \u0338 = 0 and B jk \u0338 = 0, i.e., B {i,j},L is not the null matrix.For the other direction suppose that rank(B {i,j},L ) = 1. By applying the Lindstr\u00f6m-Gessel-Viennot Lemma (Gessel and Viennot, 1985; Lindstr\u00f6m, 1973) where S is a vertex cut from anc(L) to {i, j} if and only if there exists no directed path in G m from anc(L) to {i, j} without passing through S. Moreover, equality holds in (6) for generic (almost all) choices of parameters. Since we assumed rank faithfulness in Condition (C4) we exclude cases where the inequality is strict and therefore have equality. By the definition of an m-domain graph we have that anc(L) = L. Thus, if rank(B {i,j},L ) = 1, there must be a single node k \u2208 L such that {k} is a vertex cut from L to {i, j}. But then it follows that i and j have to be partial pure children of k by the definition of an m-domain graph and by using the assumption that there are no zero-rows in B L .To prove Theorem 4.4 we need the following auxiliary lemma. Lemma A.1. Let G = (V, D) be a DAG with topologically ordered nodes V = [p] and let M be a lower triangular matrix with entries M ii \u0338 = 0 for all i = 1, . . . , p and M ij \u0338 = 0 if and only if there is a directed path j \u2192 \u2022 \u2022 \u2022 \u2192 i in G. Let Q \u03c31 and Q \u03c32 be permutation matrices. Then the matrixQ \u03c31 M Q \u03c32 is lower triangular if and only if \u03c3 2 = \u03c3 -1 1 and \u03c3 2 \u2208 S(G).Proof of Lemma A.1. By the definition of a permutation matrix, we haveEQUATIONFirst, suppose that \u03c3 2 = \u03c3 -1 1 and \u03c3 2 \u2208 S(G), and let i, j \u2208 [p] such that \u03c3 2 (i) < \u03c3 2 (j). Then, by the definition of S(G), there is no directed path j \u2192 \u2022 \u2022 \u2022 \u2192 i in the graph G and therefore we haveM ij = 0. But this means that [Q \u03c31 M Q \u03c32 ] \u03c32(i)\u03c32(j)= 0 and we conclude that the matrix Q \u03c31 M Q \u03c32 is lower triangular. Now, assume that Q \u03c31 M Q \u03c32 is lower triangular, where \u03c3 1 and \u03c3 2 are arbitrary permutations on the set [p] . Since M has no zeros on the diagonal, we haveM ii = [Q \u03c31 M Q \u03c32 ] \u03c3 -11 (i)\u03c32(i) \u0338 = 0 for all i = 1, . . . , p. It follows that \u03c3 -1 1 (i) \u2265 \u03c3 2 (i) for all i = 1, . . . , p because Q \u03c31 M Q \u03c32 is lower triangular. But this is only possible if the permutations coincide on all elements, i.e., we have \u03c3 2 = \u03c3 -1 1 . It remains to show that \u03c3 2 = \u03c3 -1 1 \u2208 S(G). For any edge j \u2192 i \u2208 D we have that M ij \u0338 = 0. Recalling Equation ( 7) this means that [Q \u03c31 M Q \u03c32 ] \u03c32(i)\u03c32(j) \u0338 = 0. But since Q \u03c31 M Q \u03c32 is lower triangular this can only be the case if \u03c3 2 (j) < \u03c3 2 (i) which proves that \u03c3 2 \u2208 S(G).Proof of Theorem 4.4. Each latent node in L has two partial pure children by Condition (C3). After removing zero-rows in Line 3 of Algorithm 2 it holds by Lemma 4.3 that rank(B * {i,j},L ) = 1 if and only if there is a latent node in L such that i and j are both partial pure children of that latent node. Hence, each tuple (i k , j k ) k\u2208L in Line 4 of Algorithm 2 consists of two partial pure children of a certain latent node. The requirement rank(B * {i k ,iq},L ) = 2 ensures that each pair of partial pure children has a different parent.By the definition of an m-domain-graph and the fact that B * = B L , for I = {i 1 , . . . , i \u2113 }, we have the factorizationEQUATIONwhere G \u2208 R D V H , A \u2208 R D HH and \u03a8 L is a signed permutation matrix. Let Q 1 and Q 2 be permutation matrices and let \u039b be a diagonal matrix with non-zero diagonal elements and let D be a diagonal matrix with entries in {\u00b11}. Then we can rewrite Equation (8) asB * I,L = Q 1 \u039b(I -A L,L ) -1 D =:M Q 2 .Now, we apply Lemma A.1. Since we assume throughout Section 4 that the nodes L are topologically ordered, the matrix M is lower triangular with no zeros on the diagonal. Moreover, by Condition (C4) we have M ij \u0338 = 0 if and only if there is a directed path j \u2192 \u2022 \u2022 \u2022 \u2192 i in G L . In Line 6 we find other permutation matrices R 1 and R 2 such thatW = R 1 B * I,L R 2 = (R 1 Q 1 )M (Q 2 R 2 )is lower triangular. Now, define the permutation matricesQ \u03c31 = R 1 Q 1 and Q \u03c32 = Q 2 R 2 .Then we have by Lemma A.1 that Q \u03c31 = Q \u22a4 \u03c32 and that \u03c3 2 \u2208 S(G L ). Hence, the matrix W factorizes asW = Q \u22a4 \u03c32 M Q \u03c32 = Q \u22a4 \u03c32 \u039b(I -A L,L ) -1 DQ \u03c32 = \u039bQ \u22a4 \u03c32 (I -A L,L ) -1 Q \u03c32 D, where \u039b and D are diagonal matrices with the entries given by permutations of the entries of \u039b and D. Lines 7 and 8 address the scaling and sign matrices \u039b and D. In particular, we have thatW \u2032 = D \u2032 Q \u22a4 \u03c32 (I -A L,L ) -1 Q \u03c32 D \u2032for another diagonal matrix D \u2032 with entries in {\u00b11}, since each entry on the diagonal of W \u2032 is equal to 1. Thus, we haveA = I -( W \u2032 ) -1 = I -(D \u2032 Q \u22a4 \u03c32 (I -A L,L ) -1 Q \u03c32 D \u2032 ) -1 = I -D \u2032 Q \u22a4 \u03c32 (I -A L,L )Q \u03c32D \u2032 = D \u2032 Q \u22a4 \u03c32 A L,L Q \u03c32 D \u2032 . Since Q \u03c32 D \u2032is a signed permutation matrix with \u03c3 2 \u2208 S(G L ), the first part of the theorem is proved. If G vk > 0 whenever v is a pure child of k, the matrix \u039b only has positive entries which means that D \u2032 is equal to the identity matrix. This proves the second part.The graph in Figure 4 is an m-domain graph corresponding to the compact version in Figure 2 Since the shared latent nodes are given by L = {1, 2}, we have have rank one. The first matrix corresponds to the partial pure children {v 1 2 , v 2 1 } in the graph in Figure 4 while the second matrix correspond to the partial pure children {v 1 3 , v 2 3 }. Note that the rank of any other 2 \u00d7 2 submatrix is generically (i.e., almost surely) equal to 2.G = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed g 1 11 g 1 12 g 1 13 0 0 g 1 21 0 g 1 23 0 0 0 g 1 32 g 1 33 g 1 34 0 g 1 41 g 1 42 g 1 43 g 1 44 0 g 2 11 0 0 0 g 2 13 g 2 21 g 2 22 0 0 g 2 23 0 g 2 32 0 0 0 g 2 41 g 2 42 0 0 g 2 43 g 2 51 g 2 52 0 0 0 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 and B = G \u2022 (I -A) -1 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed a 21 g 1 12 + g 1 11 g 1 12 g 1 13 0 0 g 1 21 0 g 1 23 0 0 a0 0 0 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 . 3 4 1 2 5 v 1 1 v 1 2 v 1 3 v 1 4 v 2 1 v 2 2 v 2 3 v 2 4 v 2 5B L = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed a 21 g 1 12 + g 1In this section, we discuss aspects of Conditions (C1)-(C3) that allow for identifiability. In particular, we discuss the necessity of pairwise different and non-Gaussian error distributions if one is not willing to make further assumptions. Moreover, we elaborate on the sparsity conditions on the mixing matrix and explain why some sparsity assumption is necessary. The left-hand side says that the marginal distributions in each domain are equal, while the right-hand side says that the joint distributions are equal. If there are m-domain graphs, such that the left-hand sides holds but the right-hand side is violated, then we say that the joint distribution is not identifiable.We assume in this section that the marginal error distributions P i , i \u2208 H are non-Gaussian and have unit variance, but are not necessarily pairwise different or non-symmetric. Then the right-hand side holds if and only if the number of shared latent nodes in each graph is equal, i.e., \u2113 = \u2113, and there is a signed permutation matrix \u03a8 such that B = B\u03a8 and P = \u03a8 \u22a4 # P . Here, the matrix \u03a8 does not necessarily have a block structure. The equivalence is implied by the identifiability of the usual, one-domain linear ICA ( see, e.g., Buchholz et al. (2022) ) together with the fact that for \u2113 \u0338 = \u2113, we have |H| \u0338 = | H| and, therefore, the distributions on the right-hand have support over different dimensional subspaces.Theorem 3.1 shows that assumptions (C1) and (C2) are sufficient for identifiability of the joint distribution. In particular, we show that they imply identifiability in a stronger sense, namely, that it follows from the left-hand side that \u2113 = \u2113 and B = B\u03a8 and P = \u03a8 \u22a4 # P for a signed permutation \u03a8 \u2208 \u03a0 with block structure. The next proposition reveals necessary conditions for identifiability. Proposition C.1. Let G m be an m-domain graph with shared latent nodes L = [\u2113], and let P X \u2208 M(G m ) with representation (B, P ). Suppose that m \u2265 2 and that everything except the assumption about pairwise different error distributions in Conditions (C1) and ( C2) is satisfied. Then, the joint distribution is not identifiable if one of the following holds:(i) There is i, j \u2208 L such that P i = P j or P i = -P j .(ii) There is i \u2208 L and j \u2208 I e for some e \u2208 [m] such that P i = P j or P i = -P j .(iii) For all e \u2208 [m] there is i e \u2208 I e such that P ie = P j f or P ie = -P i f for all e \u0338 = f .Proof. For each of the three cases, we will construct another m-domain graph G m = (H \u222a V, D) such that for suitable representations ( B, P ) of distributions in M( G m ), the left-hand side of ( 9) holds, but the right-hand side is violated.To prove the statement for case (i), let i, j \u2208 L and assume that P i = P j . We define the m-domain graph G m = ( H \u222a V, D) to be the almost same graph as G m = (H \u222a V, D), we only "swap" the roles of the latent nodes i and j on an arbitrary domain e \u2208 [m]. That is, for each v \u2208 V e , if there was an edge i \u2192 v in D, we remove that edge from D and add the edge j \u2192 v instead, and vice versa.Otherwise, the graph G m has the same structure as G m . Now, let P = P and define a the matrix B to be the same matrix as B, except for the subcolumns B Ve,i := B Ve,j and B Ve,j := B Ve,i , that is, we swapped B Ve,i and B Ve,j . Then the pair ( B, P ) is a representation of some distribution in M( G m ).Recall from the proof of Theorem 3.1 that Condition (C2) implies that the matrix B Ve,Se has full column rank. Since we only swapped columns in B Ve, Se , it still has full column rank. Moreover, observe that the left hand side of (9) is satisfied since P i = P j , that is, the marginal distributions on the single domains coincide.However, now consider another domain f \u2208 [m] and the submatricesB Ve\u222aV f ,{i,j} = B Ve,i B Ve,j B V f ,i B V f ,j and B Ve\u222aV f ,{i,j} = B Ve,j B Ve,i B V f ,i B V f ,j .Since all of the four subcolumns are nonzero and neither B Ve,j is equal to B Ve,i nor B V f ,j is equal to B V f ,i , there is no signed permutation matrix \u2126 such that B Ve\u222aV f ,{i,j} = B Ve\u222aV f ,{i,j} \u2126. Hence, there is also no larger signed permutation matrix \u03a8 such that B = B\u03a8. We conclude that the right-hand side of ( 9) is violated and the joint distribution is not identifiable. Finally, note that the above arguments also hold if P i = -P j by adding "-" signs in appropriate places.The proof for case (ii) works with exactly the same construction. That is, for i \u2208 L and j \u2208 I e we swap the roles of i and j on the domain e. Then, for any other domain f \u2208 [m], we obtain the submatricesB Ve\u222aV f ,{i,j} = B Ve,i B Ve,j B V f ,i 0 and B Ve\u222aV f ,{i,j} = B Ve,j B Ve,i B V f ,i 0 .By the same arguments as before, this shows that there is no signed permutation matrix \u03a8 such that B = B\u03a8 and, hence, the joint distribution is not identifiable.To prove case (iii), we consider a slightly different construction. However, we also assume that P ie = P i f for all e \u0338 = f , since for P ie = -P i f we only have to add some "-" signs in the following.We Then the pair ( B, P ) is a representation of some distribution in M( G m ). Moreover, each submatrix B Ve, Se is equal to B Ve,Se up to relabeling of the columns. That is, the column that is labeled by i e in B Ve,Se is now labeled by k in B Ve, Se . We define the measure P such that P H\\{k} = P H\\{k} andP k = P ie for all e \u2208 [m].Then the pair ( B, P ) is a representation of some distribution in M( G m ) and, in particular, the left hand side of ( 9) is satisfied. That is, the marginal distributions coincide on each domain. However, the number of shared latent variables in both m-domain graphs is different since we have \u2113 = \u2113 + 1. We conclude that the joint distribution is not identifiable.The proposition states that it is in most cases necessary that error distributions are pairwise different. However, in two cases the same error distributions still lead to identifiability. First, if i, j \u2208 I e , then the corresponding error distributions may be the same and the joint distribution is still identifiable. Similarly, if there are latent nodes i e in a few domains e \u2208 [m] such that the corresponding error distributions coincide, but there is at least one domain f \u2208 [m] where there is no latent node with the same error distribution, then the joint distribution is also identifiable. Both can be seen by taking the the proofs of Theorem 3.1 and Proposition C.1 together.Gaussian Errors. Without additional assumptions to those in Section 3, it is impossible to recover the joint distribution if the distributions of the errors \u03b5 i of the latent structural equation model in Equation ( 1) are Gaussian. In this case, the distribution of Z as well as the distribution of each observed random vector X e is determined by the covariance matrix only. The observed covariance matrix in domain e \u2208 [m] is given by \u03a3 e = G e Cov[Z L\u222aIe ](G e ) \u22a4 . However, knowing \u03a3 e gives no information about Z L\u222aIe other than rank(\u03a3 e ) = |L| + |I e |, that is, we cannot distinguish which latent variables are shared and which ones are domain-specific. This is formalized in the following lemma.Lemma C.2. Let \u03a3 be any d\u00d7d symmetric positive semidefinite matrix of rank p and let \u039e be another arbitrary p \u00d7 p symmetric positive definite matrix. Then there is G \u2208 R d\u00d7p such that \u03a3 = G\u039eG \u22a4 .Proof. Let \u03a3 be a d\u00d7d symmetric positive semidefinite matrix of rank p. Then, \u03a3 has a decomposition similar to the Cholesky decomposition; see e.g. Gentle (1998, Section 3.2.2). That is, there exists a unique matrix T , such that A = T T \u22a4 , where T is a lower triangular matrix with p positive diagonal elements and d -p columns containing all zeros. Define T to be the d \u00d7 p matrix containing only the non-zero columns of T .On the other hand, let \u039e be a symmetric positive definite p \u00d7 p matrix. By the usual Cholesky decomposition (Lyche, 2020, Section 4.2.1), there exists a unique p \u00d7 p lower triangular matrix L with positive diagonal elements such that \u039e = LL \u22a4 . Now, define G := T L -1 \u2208 R d\u00d7p . Then,\u03a3 = T T \u22a4 = T L -1 LL \u22a4 L -\u22a4 T \u22a4 = G\u039eG \u22a4 .Due to Lemma C.2 it is necessary to consider non-Gaussian distributions to obtain identifiability of the joint distribution.Example C.3. In the Gaussian case we cannot distinguish whether the two observed domains in Figure 5 share a latent variable or not. Said differently, the observed marginal distributions may either be generated by the mechanism defined by graph (a) or graph (b) and there is no way to distinguish from observational distributions only.Sparsity Assumptions. Let B \u2208 Im(\u03d5 Gm ) for an m-domain graph G m = (H \u222a V, D) and suppose that we are given the matrixB L = G V,L (I -A L,L ) -1, that is, we are given the submatrix with columns indexed by the shared latent nodes. Now, assume that the graph does not impose any sparsity restrictions on G V,L , which means that the setR D V L of possible matrices G V,L is equal to R |V |\u00d7|L| .Then, the set of possible matrices B L is also unrestricted, that is, B L can be any matrix in R |V |\u00d7|L| no matter the form of the matrix A L,L \u2208 R D LL . In other words, for arbitrary shared latent graphs1 2 3 V 1 V 2 (a) 1 2 3 4 V 1 V 2 (b)Figure 5 : Compact versions of two 2-domain graphs. In both graphs, both domains have two latent parents. In setup (a) there is a shared latent parent while in setup (b) there is not.G L = (L, D LL ) and arbitrary corresponding parameter matrices A L,L \u2208 R D LL , we don\'t get any restrictions on the matrix B L . Therefore, it is impossible to infer A L,L from B L .Condition (C3) requires that there are two partial pure children for every shared latent node k \u2208 L, which implies that there are 2|L| rows in G V,L in which only one entry may be nonzero. While we show in Theorem 4.4 that this condition is sufficient for identifiability of A L,L , we leave it open for future work to find a necessary condition.We adjust Algorithm 1 such that it is applicable in the empirical data setting. That is, rather than the exact distribution P X e , we have a matrix of observations X e \u2208 R de\u00d7ne in each domain e \u2208 [m]. The sample size n e might be different across domains. We denote n min = min e\u2208[m] n e and n max = max e\u2208[m] n e . For implementing linear ICA on finite samples, multiple well developed algorithms are available, e.g., FastICA (Hyv\u00e4rinen, 1999; Hyv\u00e4rinen and Oja, 2000) , Kernel ICA (Bach and Jordan, 2003) or JADE (Cardoso and Souloumiac, 1993) . Applying them, we obtain a measure P e i which is an estimator of the true measure P e i in Algorithm 1, Line 4. The remaining challenge is the matching in Line 6 of Algorithm 1. For finite samples, the distance between empirical distributions is almost surely not zero although the true underlying distributions might be equal. In this section, we provide a matching strategy based on the two-sample Kolmogorov-Smirnov test (van der Vaart and Wellner, 1996, Section 3.7) . We match two distributions if they are not significantly different. During this process, there might occur false discoveries, that is, distributions are matched that are actually not the same. We show that the probability of falsely discovering shared nodes shrinks exponentially with the number of domains.For two univariate Borel probability measures P i , P j , with corresponding cumulative distribution functions F i , F j , the Kolmogorov-Smirnov distance is given by the L \u221e -distanced KS (P i , P j ) = \u2225F i -F j \u2225 \u221e = sup x\u2208R |F i (x) -F j (x)|.The two-sample Kolmogorov-Smirnov test statistic for the null hypothesis H 0 : d KS (P e i , P f j ) = 0 is given byEQUATION)It is important to note that P e i is not an empirical measure in the classical sense since it is not obtained from data sampled directly from the true distribution P e i . In addition to the sampling error there is the uncertainty of the ICA algorithm. However, in the analysis we present here, we will neglect this error and treat P e i as an empirical measure. In this case, under H 0 , the test statistic T ( P e i , P f j ) converges in distribution to \u2225B\u2225 \u221e , where B is a Brownian bridge from 0 to 1 (van der Vaart and Wellner, 1996, Section 2.1). For a given level \u03b1 \u2208 (0, 1), we choose the critical value as c \u03b1 = inf{t : P (\u2225B\u2225 \u221e > t) \u2264 \u03b1} and reject H 0 if T ( P e i , P f j ) > c \u03b1 . Definition D.1. Let \u03b1 \u2208 (0, 1) and suppose the distributions { P e 1 , . . . , P e se } and { P f 1 , . . . , P f s f } are given for two domains e, f \u2208 [m]. Define\u2126 \u03b1 ( P e i , P f j ) = 1 if T ef ij \u2264 c \u03b1 and T ef ij = min{min k\u2208[ s f ] T ef ik , min k\u2208[ se] T ef kj }, 0 else,where T ef ij = min{T ( P e i , P f j ), T ( P e i , -P f j )}. We say that P e i , P f j are matched if \u2126 \u03b1 ( P e i , P f j ) = 1.Definition D.1 essentially states that two measures are matched if the test statistic (10) is not significantly large and the null hypothesis cannot be rejected. Taking the minimum of T ( P e i , P f j ) and T ( P e i , -P f j ) accounts for the sign indeterminacy of linear ICA. For two fixed domains e, f \u2208 [m], if it happens that the statistic T ef ij for multiple pairs (i, j) is small enough, then the pair with the minimal value of the statistic is matched. Note that one may use any other test than the Kolmogorov-Smirnov test to define a matching as in Definition D.1. We discover a shared latent node if it is matched consistently across domains. Definition D.2. Let C = (i 1 , . . . , i m ) be a tuple with m elements such that i e \u2208 [ s e ]. Then we say that C determines a shared node if \u2126 \u03b1 ( P e ie , P f i f ) = 1 for all i e , i f \u2208 C.Inferring the existence of a shared node which does not actually exist may be considered a more serious error than inferring a shared node determined by a set C, where only some components of C are wrongly matched. In the following theorem we show that the probability of falsely discovering shared nodes shrinks exponentially with the number of wrongly matched components. EQUATION)By the triangle inequality we have ( P f i f , P f i f ) \u2265 \u03ba - \u221a 2n max n min c \u03b1 =\u21d2 d KS ( P e ie , P e ie ) \u2265 \u03ba 2 - \u221a n max \u221a 2 n min c \u03b1 or d KS ( P f i f , P f i f ) \u2265 \u03ba 2 - \u221a n max \u221a 2 n min c \u03b1 . Now, consider the event e\u2208E,f \u2208[m]e<f {T ( P e ie , P f i f ) \u2264 c \u03b1 }. On this event, there cannot be two elements e, f \u2208 E such that bothd KS ( P e ie , P e ie ) < \u03ba 2 - \u221a n max \u221a 2 n min c \u03b1 and d KS ( P f i f , P f i f ) < \u03ba 2 - \u221a n max \u221a 2 n min c \u03b1 .To see this recall that E \u2286 [m]. We conclude that it must hold {d KS ( P e ie , P e ie ) \u2265 \u03ba 2 -\u221a nmax \u221a 2 nmin c \u03b1 } for all but at most one element of E. We denote this exceptional element by e * \u2208 E. Taking up (11), we get the following:EQUATIONEQUATION)The last three steps need more explanation: Equality (14) follows from the fact that domains are unpaired. That is, the distances d KS ( P e ie , P e ie ) and d KS ( P f i f , P f i f ) are pairwise independent for different domains e, f \u2208 [m]. Equality (15) is trivial since d KS ( P e ie , P e ie ) \u2265 0. Finally, Inequality (16) follows from Condition (i) and that the function g is monotonically decreasing in n. We also used the fact that |E \\ {e * }| = |E| -1.If P e i were an empirical measure in the classical sense, then Condition (i) in Theorem D.3 translates to the well-known Dvoretzky-Kiefer-Wolfowitz inequality, that is, the function g is given by g(n, x) = Algorithm 3 IdentifyJointDistributionEmpirical 1: Hyperparameters: \u03b3 > 0, \u03b1 \u2208 (0, 1). Linear ICA: Use any linear ICA algorithm to obtain a mixing matrix B e \u2208 R de\u00d7 se , where s e = rank \u03b3 (X e (X e ) \u22a4 ). Compute the matrix \u03b7e = ( B e ) \u2020 X e \u2208 R se\u00d7ne , where ( B e ) \u2020 is the Moore-Penrose pseudoinverse of B e .Scaling: Let \u2206 e be a s e \u00d7 s e diagonal matrix with entries \u2206 e ii = 1 ne [ \u03b7e ( \u03b7e ) \u22a4 ] ii . Define B e = B e (\u2206 e ) -1/2 and \u03b7 e = (\u2206 e ) -1/2 \u03b7e .Let P e be the estimated probability measure with independent marginals such that P e i is the empirical measure of the row \u03b7 e i, * . 8: end for 9: Matching: Let \u2113 be the maximal number such that there is a signed permutation matrix Q e in each domain e \u2208 [m] such that\u2126 \u03b1t ([(Q e ) \u22a4 # P e ] i , [(Q f ) \u22a4 # P f ] i ) = 1for all i = 1, . . . , \u2113 and all f \u0338 = e, where \u03b1 t = \u03b1/t with t = 2 e<f s e s f . Let L = {1, . . . , \u2113}.10: Construct the matrix B and the tuple of probability measures P given byB = \uf8eb \uf8ec \uf8ec \uf8ed [ B 1 Q 1 ] L [ B 1 Q 1 ] [ s1]\\ L . . . . . . [ B m Q m ] L [ B m Q m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f8 and P = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed [(Q 1 ) \u22a4 # P 1 ] L [(Q 1 ) \u22a4 # P 1 ] [ s1]\\ L . . . [(Q m ) \u22a4 # P m ] [ sm]\\ L \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8.11: return ( \u2113, B, P ).Algorithm 4 IdentifySharedGraphEmpirical 1: Hyperparameters: \u03b3 > 0.2: Input: Matrix B * \u2208 R |V |\u00d7\u2113 . 3: Output: Parameter matrix A \u2208 R \u2113\u00d7\u2113 . 4: Remove rows B * i,L with \u2225B * i,L \u2225 2 \u2264 \u03b3 from the matrix B * . 5: Find tuples (i k , j k ) k\u2208L with the smallest possible scores \u03c3 min (B * {i k ,j k },L ) such that (i) i k \u0338 = j k for all k \u2208 L and {i k , j k } \u2229 {i q , j q } = \u2205 for all k, q \u2208 L such that k \u0338 = q and (ii) \u03c3 min (B * {i k ,iq},L )| > \u03b3 for all k, q \u2208 L such that k \u0338 = q. 6: Let I = {i 1 , . . . , i \u2113 } and consider the matrix B * I,L \u2208 R \u2113\u00d7\u2113 . 7: Find two permutation matrices R 1 and R 2 such that W = R 1 B * I,L R 2 is as close as possible to lower triangular. This can be measured, for example, by using i<j W 2 ij . 8: Multiply each column of W by the sign of its corresponding diagonal element. This yields a new matrix W with all diagonal elements positive. 9: Divide each row of W by its corresponding diagonal element. This yields a new matrix W \u2032 with all diagonal elements equal to one. 10: Compute A = I -( W \u2032 ) -1 . 11: return A.2 exp(-2nx 2 ). Given a tuple C = (i 1 , . . . , i m ) that defines a shared node, Condition (ii) is an assumption on the number of wrongly matched components. The most extreme case is when the shared node does not actually exist and all components are wrongly matched. That is, the measures P e ie and P f i f are matched even though d KS (P e ie , P f i f ) \u0338 = 0 and d KS (P e ie , -P f i f ) \u0338 = 0 for all e, f \u2208 [m]. On the other hand, if |E| \u226a m, then C determines a shared node where the majority of the components are correctly matched.If g(n, x) \u2192 0 for n \u2192 \u221e and x > 0, the statement of the theorem becomes meaningful under the constraint \u221a n max /n min \u2192 0. In this case, the probability that a given tuple C with wrong components E determines a shared node goes to zero for large sample sizes n min . As noted, the probability of falsely discovering a shared node decreases exponentially with the number of wrongly matched components |E|. In the extreme case, this means that the probability of falsely discovering shared nodes with all components wrongly matched, i.e., E = [m], decreases exponentially with the number of domains m.Theorem D.3 also tells us that the probability of falsely matching two measures P e i and P f j becomes zero if the sample size grows to infinity and the linear ICA algorithm is consistent. However, with finite samples we might fail to match two measures where the underlying true measures are actually the same, i.e., we falsely reject the true null hypothesis H 0 . Thus, we might be overly conservative in detecting shared nodes due to a high family-wise error rate caused by multiple testing. We suggest to correct the level \u03b1 to account for the amount of tests carried out. One possibility is to apply a Bonferroni-type correction. The total number of tests is given by t = 2 e<f s e s f . This means that an adjusted level is given by \u03b1 t = \u03b1/t and instead of the critical value c \u03b1 we consider the adjusted critical value c \u03b1t .Algorithm 3 is the finite sample version of Algorithm 1 with the matching \u2126 \u03b1 defined in Definition D.1. To determine the number of independent components for the linear ICA step in each domain, we need to check the rank(X e (X e ) \u22a4 ). We specify the rank of a matrix M as number of singular values which are larger than a certain threshold \u03b3 and denote it by rank \u03b3 (M ).In Algorithm 4 we also provide a finite sample version of Algorithm 2 where we only have the approximation B \u22c6 = B L \u2248 B L \u03a8 L for a signed permutation matrix \u03a8 L . For a matrix M , we denote by \u03c3 min (M ) the smallest singular value.We specify L = {1, 2, 3}, I 1 = {4, 5}, I 2 = {6, 7} and I 3 = {8, 9}. Note that the set I 3 does not exist if the number of domains is m = 2. The error distributions in all simulations are specified as follows if not stated otherwise.L: \u03b5 1 \u223c Beta(2, 3), \u03b5 2 \u223c Beta(2, 5), \u03b5 3 \u223c \u03c7 2 4 , I 1 : \u03b5 4 \u223c Gumbel(0, 1), \u03b5 5 \u223c LogNormal(0, 1), I 2 : \u03b5 6 \u223c Weibull(1, 2), \u03b5 7 \u223c Exp(0.1),I 3 : \u03b5 8 \u223c SkewNormal(6), \u03b5 9 \u223c SkewNormal(12),where the overline means that each distribution is standardized to have mean 0 and variance 1. Figure 6 shows histograms of the empirical distributions.In this section, we make additional experiments. First, we consider another setup where all our assumptions are satisfied but we have more domains and more shared latent variables. Then, we also consider two setups where some of our assumptions are not satisfied.Different Setup. We make additional experiments on a similar scale as in Section 5, but with more shared nodes and less domain specific nodes. This time, we consider \u2113 = 5 shared latent nodes and |I e | = 1 domain-specific latent node in each domain. Moreover, we also consider m = 4 domains. Figure 7 shows the results where the scores are equivalent as in the main paper. Once again, we see that the estimation error for the matrices B L and A L,L decreases with increasing sample size. This supports our proof of concept and shows that the adapted algorithms are consistent for recovering B L and A L,L from finite samples.Violated Assumptions. We consider the same setup as in the main paper in Section 5 with l = 3 shared latent nodes, but we fix the number of domains to m = 3. In this experiment, we compare the results where data was generated such that all our assumptions are satisfied with two setups where we violate some of the assumptions. In the first setup, we violate Condition (C1) that requires pairwise different error distributions. We specify the error distributions as follows.L: \u03b5 1 \u223c Beta(2, 3), \u03b5 2 \u223c Beta(2, 5), \u03b5 3 \u223c \u03c7 2 4 , I 1 : \u03b5 4 \u223c Beta(2, 3), \u03b5 5 \u223c LogNormal(0, 1), I 2 : \u03b5 6 \u223c Beta(2, 5), \u03b5 7 \u223c LogNormal(0, 1),I 3 : \u03b5 8 \u223c \u03c7 2 4 , \u03b5 9 \u223c LogNormal(0, 1),where, as before, the overline means that each distribution is standardized to have mean 0 and variance 1. In the second setup, we do not change the error distributions but we violate Condition (C3) that requires two partial pure children per shared latent node. In this experiment, we do not make any sparsity assumptions on the mixing matrix G V,L .Figure 8 shows the results of our experiments. As expected, we see in (a) and (b) that identifyability of the joint distribution fails if we do not require pairwise different error distributions. Recovering the joint distribution still works well in the second setup where we violate the partial pure children conditions. However, identifying the shared latent graph is impossible in this setup, as we explained in Appendix C. This is supported by the experimental results displayed in Figure 8 (c), where we can see that recovery of the shared latent graph does not work when the partial pure children assumption is not satisfied.We see many directions for future work, which include the following.\u2022 Our work and algorithms rely on linear ICA. It would be interesting to study a more direct approach to recover the joint distribution and the causal graph. This might potentially be done by testing certain constraints implied by the model similar as the developments in the LiNGAM literature; see e.g. Shimizu et al. (2011) and Wang and Drton (2020) . \u2022 Our results require non-Gaussianity and that both the latent structural equation model and the mixing functions are linear. We consider the linear setup as a basis for any subsequent study of nonlinear cases. For example, recent advances in non-linear ICA allow identifiability of up to linear transformations, see e.g. Khemakhem et al. (2020) , Buchholz et al. (2022) and Roeder et al. (2021) . Identifiability of a causal representation might then be obtained from identifiability results for the linear model. \u2022 Our sufficient condition for identifiability of the shared latent graph requires two partial pure children per shared latent node. In this regard, it would also be interesting to study necessary conditions; c.f. our discussion in Appendix C. \u2022 This work focused purely on the observational case. However, considering interventional data can be expected to permit relaxing some conditions in both of the key steps, i.e., recovering the joint distribution and the shared latent graph. For example, recent work shows that interventional data allow for identification of the latent graph without sparsity constraints in a single-domain setup (Squires et al., 2023; Ahuja et al., 2023) . Extending this to the multi-domain setup is an interesting problem for future work. \u2022 It would be interesting to study the statistical properties of our setup such as theoretical bounds on the accuracy of recovering the matrices B and A L,L as well as developing algorithms that meet these bounds. Currently, our adapted algorithms for finite samples determine the rank of a matrix by using a threshold for singular values. The algorithms depend on the choice of this parameter and it would be worth studying optimal choices. Moreover, one might consider different methods for determining the rank of a matrix. \u2022 There might be different matching strategies of the estimated error distributions in the finite sample setting. For example, instead of matching pairwise consistently across all domains as we propose in Appendix D, one might find an optimal matching by solving a linear program.'
experiments_neurips_2023_1 = (
    experiments_neurips_2023_1_1 + "\n" + experiments_neurips_2023_1_2
)
conclusion_neurips_2023_1 = 'Conclusion: This work introduces the problem of causal representation learning from unpaired multi-domain observations, in which multiple domains provide complementary information about a set of shared latent nodes that are the causal quantities of primary interest. For this problem, we laid out a setting in which we can provably identify the causal relations among the shared latent nodes. To identify the desired causal structure, we proposed a two-step approach where we first make use of linear ICA in each domain separately and match the recovered error distributions to identify shared nodes and the joint distribution of the domains. In the second step, we identify the causal structure among the shared latent variables by testing rank deficiencies in the "overall mixing matrix" B. To the best of our knowledge, our guarantees are the first principled identifiability results for shared causal representations in a general, unpaired multi-domain setting.'

paper_text_neurips_2023_1 = f"{title_neurips_2023_1}\n {abstract_neurips_2023_1}\n {introduction_neurips_2023_1}\n {related_work_neurips_2023_1}\n{method_neurips_2023_1}\n{experiments_neurips_2023_1} \n {conclusion_neurips_2023_1}"

review_1_neurips_2023_1 = "paper_summary: This work tackles the problem of learning the latent causal structure from multiple unpaired domains. Under a linear non-Gaussian condition, this work presents the identifiability guarantees for the joint distribution over the domains and the causal structure within the shared latent partition. Synthetic data experiments are presented to validate the theory. \n \
                strength_weakness: 1. The linear assumptions: practical multi-domain (modal) data-generating processes are often highly nonlinear, e.g., images and text. The applicability of the linear assumption may not be as appealing.\n2. The heterogeneous noise distributions: pairwise distinct exogenous distributions appear a strong assumption to me and can potentially oversimplify the technical challenge. I would be interested in learning about the necessity of such an assumption.\n \
                questions: I would like to learn about the authors' response to the weaknesses listed above, which may give me a clearer perspective on the paper's contribution.\n \
                limitations: Please see the weakness section. \n \
                score: 6 \n \
                confidence: 3 \n \
                correctness: 3 \n \
                clarity: 3 \n \
                impact: 3"


review_2_neurips_2023_1 = "paper_summary: - The paper considers causal representation learning from unpaired multi-domain data, with latent variables both shared and specific to domains.\n- Its key contribution is a new identifiability result for linear causal models with non-Gaussian noise, linear mixing function, and a number of other assumptions.\n- In addition, the authors develop a practical representation learning algorithm for this setting and demonstrate it on toy data.\n\nI've read the authors' rebuttal. They have addressed my concerns adequately. \n \
                strength_weakness: - The contribution made here has only one real weakness, and that is the host of strong assumptions underlying the identifiability result: 1D causal variables, linear causal model, no causal effects from shared to domain-specific latents, non-symmetric error distributions, different error distributions for each variable, linear mixing function, full-rank mixing function, observed variables include sufficient \"partially pure children\", and the list goes on. To put it bluntly, this list makes me wonder if this identifiability result present progress on the road to algorithms that work in practice on interesting real-world datasets.\n    - Of course, strong statements such as CRL identifiability require strong inputs, but these need not be in the form of model assumptions, they could also come from the data side. Perhaps it is a bit out of scope for this paper, but I would be curious if the availability of *interventional* data or some other form of auxiliary data would allow the relaxation of some of these model assumptions.\n    - While the authors do a good job in providing an intuition for why these assumptions are needed, I would like to know if there are any real-world problems that satisfy them all. This is partially discussed for single-cell data and a few of these assumptions, but could the authors provide a more complete example that ideally satisfies all assumptions?\n- There are no experiments to speak of, though I also don't think that all papers need experiments. \n \
                questions: - See above.\n- Very minor suggestion: there are a few instances of `\\citet{}` that should be a `\\citep{}`. It speaks for the quality of the writing that I can't think of any other comments here. \n \
                limitations: - The paper is very clear about the (many) assumptions in the theory. It also openly acknowledges the limitations of the experiments.\n- I do not see any particular need for an extensive discussion of societal impacts. \n \
                score: 7 \n \
                confidence: 4 \n \
                correctness: 4 \n \
                clarity: 4 \n \
                impact: 3 \n "

review_3_neurips_2023_1 = "paper_summary: In this paper, the authors address unpaired multi-domain causal representation. In detail, the authors learn the representations of the observed data from different domains that consist of causally. To achieve this, the authors consider the data generation process where the relationship between the latent variables is linear. Based on this generation process, they prove that the joint distribution of observed data and the shared causal structures of latent variables are identifiable. \n \
                strength_weakness: 1.\tThere are several works about causal discovery with latent variables under linear and multi-domain case like [1], which also considers the shared causal structure among latent variables and provide identification guarantees. It is suggested that the authors should discuss these works.  \n2.\tMoreover, the authors discuss several works about domain translation between unpaired data and claim that none of these works have rigorous identifiability guarantees. However, Multi-domain image generation, image translation, and domain adaptation belong to the proposed setting, and [2][3] have addressed the multi-domain causal representation learning problem recently. And the authors do not consider these works. It is noted that [2][3] considers the multi-domain causal representation learning with nonlinear transformation, which seems to be more general than the proposed setting.  \n3.\tAs for the identification of joint distributions, it is not clear why the identification of $l, B$, and $P$ can identify the joint distributions of observed data from multi-domains.   \n4.\tIn section 3, the authors assume that the distribution of errors is non-Gaussian for the identification of linear ICA by not allowing asymmetric distribution. But some distribution like the Laplace distribution is symmetric and they can also satisfy the identification of linear ICA.\n5.\tAccording to this paper, the authors consider the structure of latent variables to be linear but flexible. In the simulation experiment, the authors only consider three shared latent variables, it is suggested that the authors should consider more latent variables and different structures.  \n6.\tBesides, it is suggested that the authors should consider more compared methods and employ other metrics like recall, and precisions to evaluate the performance of causal discovery.  \n\n[1] Causal Discovery with Multi-Domain LiNGAM for Latent Factors  \n[2] Multi-domain image generation and translation with identifiability guarantees  \n[3] partial disentanglement for domain adaptation \n \
                questions: Please refer weaknesses, \n \
                limitations: Please refer weaknesses \n \
                score: 6 \n \
                confidence: 3 \n \
                correctness: 3 \n \
                clarity: 3 \n \
                impact: 2 "


review_4_neurips_2023_1 = "paper_summary: The authors study the setting of unsupervised learning where observations belong to several domains, and we only observe the marginal distribution of each domain. A set of latent variables generates the observations, where a subset of latents are shared across domains. The authors provide the first identification results in this setting, assuming that the latents follow a linear SCM, and the observations are an injective linear transformation of the latents. With this model, identifying the (unobserved) joint distribution of the observations equates to identifying the latents. The mapping between the exogenous noises and the observations, as well as the distributions over the exogenous noises, are identified up to signed block permutation. With additional conditions, the authors also identify the causal graph for the latents up to a signed permutation consistent with the topological ordering of the latents. The authors validate their claims with a synthetic data experiment. \n \
                strength_weakness: This paper could be improved with more context on how they are extending existing identification results to achieve theirs. Currently, the authors mention which existing results are being used, but do not provide an intuitive description on why they need to be extended, and how they do so.\n\nThe notation could be improved. Capital letters are used to denote matrices, probability measures, vector- and scalar-valued random variables, sets of nodes, and sets of edges. It would improve readability if you used font styles (e.g. lower-case bold for vectors) to differentiate them. \n \
                questions: This paper makes two contributions. The first is to extend identifiable single-domain linear ICA to the multi-domain setting. The second is to extend single-domain graph identifiability to the multi-domain setting. In both cases, can you intuitively describe what about the multi-domain setting prevents the existing results from holding automatically?\n\nFor example, if there are no shared latents, the joint can be identified by trivially applying linear ICA separately to each domain. What is the difficulty imposed by a subset of latents being shared, and how is this circumvented? \n \
                limitations: The authors made it explicit that this work is primarily about identifiability, and less about scalable algorithms and evaluation on realistic datasets. \n \
                score: 7 \n \
                confidence: 3 \n \
                correctness: 4 \n \
                clarity: 3 \n \
                impact: 4"


review_text_neurips_2023_1 = f"Review 1: {review_1_neurips_2023_1}\n Review 2: {review_2_neurips_2023_1}\n Review 3: {review_3_neurips_2023_1} \n Review 4: {review_4_neurips_2023_1}"


FEW_SHOT_PAPERS_ICLR = [paper_text_iclr_2023_1, paper_text_iclr_2023_2]
FEW_SHOT_REVIEWS_ICLR = [review_text_iclr_2023_1, review_text_iclr_2023_2]
FEW_SHOT_PAPERS_NEURIPS = [paper_text_neurips_2023_1]
FEW_SHOT_REVIEWS_NEURIPS = [review_text_neurips_2023_1]


# PROMPTS

TEMPLATE_INSTRUCTIONS = """
Respond in the following format:

THOUGHT:
<THOUGHT>

REVIEW JSON:
```json
<JSON>
```

In <THOUGHT>, first briefly discuss your intuitions and reasoning for the evaluation.
Detail your high-level arguments, necessary choices and desired outcomes of the review.
Do not make generic comments here, but be specific to your current paper.
Treat this as the note-taking phase of your review.

In <JSON>, provide the review in JSON format with the following fields in the order:
- "Summary": A summary of the paper content and its contributions.
- "Strengths": A list of strengths of the paper.
- "Weaknesses": A list of weaknesses of the paper.
- "Originality": A rating from 1 to 4 (low, medium, high, very high).
- "Quality": A rating from 1 to 4 (low, medium, high, very high).
- "Clarity": A rating from 1 to 4 (low, medium, high, very high).
- "Significance": A rating from 1 to 4 (low, medium, high, very high).
- "Questions": A set of clarifying questions to be answered by the paper authors.
- "Limitations": A set of limitations and potential negative societal impacts of the work.
- "Ethical Concerns": A boolean value indicating whether there are ethical concerns.
- "Soundness": A rating from 1 to 4 (poor, fair, good, excellent).
- "Presentation": A rating from 1 to 4 (poor, fair, good, excellent).
- "Contribution": A rating from 1 to 4 (poor, fair, good, excellent).
- "Overall": A rating from 1 to 10 (very strong reject to award quality).
- "Confidence": A rating from 1 to 5 (low, medium, high, very high, absolute).
- "Decision": A decision that has to be one of the following: Accept, Reject.

"""

ICLR_FORM = (
    """
             ## Review Form
Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.
When writing your review, please keep in mind that after decisions have been made, reviews and meta-reviews of accepted papers and opted-in rejected papers will be made public. 

1. Summary: Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.
  - Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions:
  - Originality: Are the tasks or methods new? Is the work a novel combination of well-known techniques? (This can be valuable!) Is it clear how this work differs from previous contributions? Is related work adequately cited
  - Quality: Is the submission technically sound? Are claims well supported (e.g., by theoretical analysis or experimental results)? Are the methods used appropriate? Is this a complete piece of work or work in progress? Are the authors careful and honest about evaluating both the strengths and weaknesses of their work
  - Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)
  - Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?

2. Questions: Please list up and carefully describe any questions and suggestions for the authors. Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This can be very important for a productive rebuttal and discussion phase with the authors.  

3. Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work? If not, please include constructive suggestions for improvement.
In general, authors should be rewarded rather than punished for being up front about the limitations of their work and any potential negative societal impact. You are encouraged to think through whether any critical points are missing and provide these as feedback for the authors.

4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review. For guidance on when this is appropriate, please review the NeurIPS ethics guidelines.

5. Soundness: Please assign the paper a numerical rating on the following scale to indicate the soundness of the technical claims, experimental and research methodology and on whether the central claims of the paper are adequately supported with evidence.
  4: excellent
  3: good
  2: fair
  1: poor

6. Presentation: Please assign the paper a numerical rating on the following scale to indicate the quality of the presentation. This should take into account the writing style and clarity, as well as contextualization relative to prior work.
  4: excellent
  3: good
  2: fair
  1: poor

7. Contribution: Please assign the paper a numerical rating on the following scale to indicate the quality of the overall contribution this paper makes to the research area being studied. Are the questions being asked important? Does the paper bring a significant originality of ideas and/or execution? Are the results valuable to share with the broader NeurIPS community.
  4: excellent
  3: good
  2: fair
  1: poor

8. Overall: Please provide an "overall score" for this submission. Choices: 
  10: Strong accept should be highlighted at the conference: Technically flawless paper with groundbreaking impact on one or more areas of AI, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.
  8: Accept, good paper: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high-to-excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.
  5: Marginally below the acceptance threshold: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.
  3: Reject, not good enough: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.
  1: Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations

9. Confidence:  Please provide a "confidence score" for your assessment of this submission to indicate how confident you are in your evaluation. Choices:
  5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
  4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
  3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
  2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
  1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.
"""
    + TEMPLATE_INSTRUCTIONS
)

NEURIPS_FORM = (
    """
## Review Form
Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.
When writing your review, please keep in mind that after decisions have been made, reviews and meta-reviews of accepted papers and opted-in rejected papers will be made public. 

1. Summary: Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.
  - Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions:
  - Originality: Are the tasks or methods new? Is the work a novel combination of well-known techniques? (This can be valuable!) Is it clear how this work differs from previous contributions? Is related work adequately cited
  - Quality: Is the submission technically sound? Are claims well supported (e.g., by theoretical analysis or experimental results)? Are the methods used appropriate? Is this a complete piece of work or work in progress? Are the authors careful and honest about evaluating both the strengths and weaknesses of their work
  - Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)
  - Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?

2. Questions: Please list up and carefully describe any questions and suggestions for the authors. Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This can be very important for a productive rebuttal and discussion phase with the authors.  

3. Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work? If not, please include constructive suggestions for improvement.
In general, authors should be rewarded rather than punished for being up front about the limitations of their work and any potential negative societal impact. You are encouraged to think through whether any critical points are missing and provide these as feedback for the authors.

4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review. For guidance on when this is appropriate, please review the NeurIPS ethics guidelines.

5. Soundness: Please assign the paper a numerical rating on the following scale to indicate the soundness of the technical claims, experimental and research methodology and on whether the central claims of the paper are adequately supported with evidence.
  4: excellent
  3: good
  2: fair
  1: poor

6. Presentation: Please assign the paper a numerical rating on the following scale to indicate the quality of the presentation. This should take into account the writing style and clarity, as well as contextualization relative to prior work.
  4: excellent
  3: good
  2: fair
  1: poor

7. Contribution: Please assign the paper a numerical rating on the following scale to indicate the quality of the overall contribution this paper makes to the research area being studied. Are the questions being asked important? Does the paper bring a significant originality of ideas and/or execution? Are the results valuable to share with the broader NeurIPS community.
  4: excellent
  3: good
  2: fair
  1: poor

8. Overall: Please provide an "overall score" for this submission. Choices: 
  10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas of AI, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.
  9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI and excellent impact on multiple areas of AI, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.
  8: Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high-to-excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.
  7: Accept: Technically solid paper, with high impact on at least one sub-area of AI or moderate-to-high impact on more than one area of AI, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.
  6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.
  5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.
  4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.
  3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and incompletely addressed ethical considerations.
  2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.
  1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations

9. Confidence:  Please provide a "confidence score" for your assessment of this submission to indicate how confident you are in your evaluation. Choices:
  5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
  4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
  3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
  2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
  1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.
"""
    + TEMPLATE_INSTRUCTIONS
)


REVIEWER_SYSTEM_PROMPT_BASE = (
    "You are an AI researcher who is reviewing a paper that was submitted to a prestigious ML venue."
    "Be critical and cautious in your decision."
)

REVIEWER_SYSTEM_PROMPT_NEG = (
    REVIEWER_SYSTEM_PROMPT_BASE
    + "If a paper is bad or you are unsure, give it bad scores and reject it."
)


META_REVIEWER_SYSTEM_PROMPT = """You are an Area Chair at a machine learning conference.
You are in charge of meta-reviewing a paper that was reviewed by {reviewer_count} reviewers.
Your job is to aggregate the reviews into a single meta-review in the same format.
Be critical and cautious in your decision, find consensus, and respect the opinion of all the reviewers."""


REVIEWER_REFLECTION_PROMPT = """Round {current_round}/{num_reflections}. In your thoughts, first carefully consider the accuracy and soundness of the review you just created.
Include any other factors that you think are important in evaluating the paper.
Ensure the review is clear and concise, and the JSON is in the correct format.
Do not make things overly complicated.
In the next attempt, try and refine and improve your review.
Stick to the spirit of the original review unless there are glaring issues.

Respond in the same format as before:
THOUGHT:
<THOUGHT>

REVIEW JSON:
```json
<JSON>
```
"""
